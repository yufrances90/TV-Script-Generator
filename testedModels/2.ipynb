{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"2.ipynb","provenance":[{"file_id":"18355HaKcdEjM7ItsN3ypHfAf61cLo0qD","timestamp":1590861568376},{"file_id":"18QvIHicyw5C-0ilk7hjZ_-ntievwHAsS","timestamp":1590807306832}],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"9kyuHNH2gO4b","colab_type":"text"},"source":["# 1. Import Modules"]},{"cell_type":"code","metadata":{"id":"NCgQckobgO4f","colab_type":"code","colab":{}},"source":["import numpy as np\n","import torch\n","import os\n","import pickle\n","\n","import torch.nn.functional as F\n","\n","from torch import nn\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","import helper\n","# import problem_unittests as tests"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LMa3EQndgmKu","colab_type":"code","outputId":"e96d3456-7da8-48e1-ca7b-d6df7325b224","executionInfo":{"status":"ok","timestamp":1590894580347,"user_tz":420,"elapsed":21155,"user":{"displayName":"Frances Yu","photoUrl":"","userId":"14628227363021680713"}},"colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["from google.colab import drive\n","\n","drive.mount('/content/gdrive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lU0EIV4Hij3K","colab_type":"code","colab":{}},"source":["SPECIAL_WORDS = {'PADDING': '<PAD>'}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6LSlg_Z-gO4v","colab_type":"text"},"source":["# 2. Explore the Data"]},{"cell_type":"code","metadata":{"id":"9M83fuYuhyhj","colab_type":"code","colab":{}},"source":["def load_data(path):\n","    \"\"\"\n","    Load Dataset from File\n","    \"\"\"\n","    input_file = os.path.join(path)\n","    with open(input_file, \"r\") as f:\n","        data = f.read()\n","\n","    return data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cGM_GDt5iO1g","colab_type":"code","colab":{}},"source":["def preprocess_and_save_data(dataset_path, token_lookup, create_lookup_tables):\n","    \"\"\"\n","    Preprocess Text Data\n","    \"\"\"\n","    text = load_data(dataset_path)\n","    \n","    # Ignore notice, since we don't use it for analysing the data\n","    text = text[81:]\n","\n","    token_dict = token_lookup()\n","    for key, token in token_dict.items():\n","        text = text.replace(key, ' {} '.format(token))\n","\n","    text = text.lower()\n","    text = text.split()\n","\n","    vocab_to_int, int_to_vocab = create_lookup_tables(text + list(SPECIAL_WORDS.values()))\n","    int_text = [vocab_to_int[word] for word in text]\n","    pickle.dump((int_text, vocab_to_int, int_to_vocab, token_dict), open('gdrive/My Drive/Lab/preprocess.p', 'wb'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xwo1eoaEiwHx","colab_type":"code","colab":{}},"source":["def load_preprocess():\n","    \"\"\"\n","    Load the Preprocessed Training data and return them in batches of <batch_size> or less\n","    \"\"\"\n","    return pickle.load(open('gdrive/My Drive/Lab/preprocess.p', mode='rb'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SZF87z9ZjO44","colab_type":"code","colab":{}},"source":["def save_model(filename, decoder):\n","    torch.save({'state_dict': decoder.state_dict()}, filename)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U516D_QlgO42","colab_type":"code","colab":{}},"source":["data_dir = 'gdrive/My Drive/Lab/data/Seinfeld_Scripts.txt'\n","text = load_data(data_dir)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_XSjkzP9gO5C","colab_type":"code","outputId":"c4a93233-f61c-46ba-9797-85a0082ac730","executionInfo":{"status":"ok","timestamp":1590894606661,"user_tz":420,"elapsed":667,"user":{"displayName":"Frances Yu","photoUrl":"","userId":"14628227363021680713"}},"colab":{"base_uri":"https://localhost:8080/","height":315}},"source":["view_line_range = (0, 10)\n","\n","print('Dataset Stats')\n","print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n","\n","lines = text.split('\\n')\n","print('Number of lines: {}'.format(len(lines)))\n","word_count_line = [len(line.split()) for line in lines]\n","print('Average number of words in each line: {}'.format(np.average(word_count_line)))\n","\n","print()\n","print('The lines {} to {}:'.format(*view_line_range))\n","print('\\n'.join(text.split('\\n')[view_line_range[0]:view_line_range[1]]))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Dataset Stats\n","Roughly the number of unique words: 46367\n","Number of lines: 109233\n","Average number of words in each line: 5.544240293684143\n","\n","The lines 0 to 10:\n","jerry: do you know what this is all about? do you know, why were here? to be out, this is out...and out is one of the single most enjoyable experiences of life. people...did you ever hear people talking about we should go out? this is what theyre talking about...this whole thing, were all out now, no one is home. not one person here is home, were all out! there are people trying to find us, they dont know where we are. (on an imaginary phone) did you ring?, i cant find him. where did he go? he didnt tell me where he was going. he must have gone out. you wanna go out you get ready, you pick out the clothes, right? you take the shower, you get all ready, get the cash, get your friends, the car, the spot, the reservation...then youre standing around, what do you do? you go we gotta be getting back. once youre out, you wanna get back! you wanna go to sleep, you wanna get up, you wanna go out again tomorrow, right? where ever you are in life, its my feeling, youve gotta go. \n","\n","jerry: (pointing at georges shirt) see, to me, that button is in the worst possible spot. the second button literally makes or breaks the shirt, look at it. its too high! its in no-mans-land. you look like you live with your mother. \n","\n","george: are you through? \n","\n","jerry: you do of course try on, when you buy? \n","\n","george: yes, it was purple, i liked it, i dont actually recall considering the buttons. \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cMZ6xJoYgO5R","colab_type":"text"},"source":["# 3. Implement Pre-processing Functions"]},{"cell_type":"markdown","metadata":{"id":"S_HCdCzDgO5S","colab_type":"text"},"source":["### Lookup Table"]},{"cell_type":"code","metadata":{"id":"zH2DNdwGgO5V","colab_type":"code","colab":{}},"source":["def create_lookup_tables(text):\n","    \"\"\"\n","    Create lookup tables for vocabulary\n","    :param text: The text of tv scripts split into words\n","    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n","    \"\"\"\n","    \n","    vocab_to_int = dict()\n","    int_to_vocab = dict()\n","    \n","    sorted_word_set = sorted(set(text))\n","    \n","    for i, word in enumerate(sorted_word_set):\n","        vocab_to_int[word] = i\n","        int_to_vocab[i] = word\n","    \n","    return (vocab_to_int, int_to_vocab)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kQXrG9uAgO5f","colab_type":"code","outputId":"f17d9b35-3c5b-4418-c63a-6668c4b33fc8","colab":{}},"source":["tests.test_create_lookup_tables(create_lookup_tables)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Tests Passed\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IxzvAd5rgO5r","colab_type":"text"},"source":["### Tokenize Punctuation"]},{"cell_type":"code","metadata":{"id":"kYD7Qf70gO5r","colab_type":"code","colab":{}},"source":["def token_lookup():\n","    \"\"\"\n","    Generate a dict to turn punctuation into a token.\n","    :return: Tokenized dictionary where the key is the punctuation and the value is the token\n","    \"\"\"\n","    token_dict = dict()\n","    \n","    punctuation_list = [\n","        '.', \n","        ',', \n","        '\"', \n","        ';', \n","        '!', \n","        '?', \n","        '(', \n","        ')',\n","        '-',\n","        '\\n'\n","    ]\n","    \n","    token_list = [\n","        '||Period||', \n","        '||Comma||', \n","        '||Quotation_Mark||', \n","        '||Semicolon||', \n","        '||Exclamation_Mark||', \n","        '||Question_Mark||', \n","        '||Left_Parentheses||', \n","        '||Right_Parentheses||', \n","        '||Dash||', \n","        '||Return||'\n","    ]\n","    \n","    for (punctuation, token) in zip(punctuation_list, token_list):\n","        token_dict[punctuation] = token\n","        \n","    return token_dict"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gMSDBfGXgO50","colab_type":"code","outputId":"3bc34a9f-4255-4e94-a374-c989867b755c","colab":{}},"source":["tests.test_tokenize(token_lookup)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Tests Passed\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"niBXV40EgO58","colab_type":"text"},"source":["## Pre-process & Save Data"]},{"cell_type":"code","metadata":{"id":"wT6LRcoIgO5-","colab_type":"code","colab":{}},"source":["preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2L3nJN2bgO6G","colab_type":"text"},"source":["# **************** #1 Check Point ****************"]},{"cell_type":"code","metadata":{"id":"b5HuaioygO6H","colab_type":"code","colab":{}},"source":["int_text, vocab_to_int, int_to_vocab, token_dict = load_preprocess()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"URxy4KAmgO6N","colab_type":"text"},"source":["### Check Access to GPU"]},{"cell_type":"code","metadata":{"id":"OvnuuJZVgO6P","colab_type":"code","colab":{}},"source":["train_on_gpu = torch.cuda.is_available()\n","if not train_on_gpu:\n","    print('No GPU found. Please use a GPU to train your neural network.')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7cG8j4K-gO6Y","colab_type":"text"},"source":["### Batch Input Data"]},{"cell_type":"code","metadata":{"id":"cvScOkgJgO6Z","colab_type":"code","colab":{}},"source":["def batch_data(words, sequence_length, batch_size):\n","    \"\"\"\n","    Batch the neural network data using DataLoader\n","    :param words: The word ids of the TV scripts\n","    :param sequence_length: The sequence length of each batch\n","    :param batch_size: The size of each batch; the number of sequences in a batch\n","    :return: DataLoader with batched data\n","    \"\"\"\n","    \n","    ###\n","    \n","    words_len = len(words)\n","    \n","    batch_size_total = batch_size * sequence_length\n","    \n","    n_batches = words_len // batch_size_total\n","    \n","    words = words[:n_batches * batch_size_total]\n","    \n","    x,y = [],[]\n","    \n","    for idx in range(0,len(words) - sequence_length):\n","        x.append(words[idx:idx + sequence_length])\n","        y.append(words[idx+sequence_length])\n","        \n","    feature_arr = np.asarray(x)\n","    target_arr = np.asarray(y)\n","        \n","    ###\n","    \n","    feature_tensors = torch.from_numpy(feature_arr)\n","    target_tensors = torch.from_numpy(target_arr)\n","    \n","    ###\n","    \n","    data = TensorDataset(feature_tensors, target_tensors)\n","    data_loader = torch.utils.data.DataLoader(\n","        data, \n","        shuffle=True,\n","        batch_size=batch_size\n","    )\n","        \n","        \n","    return data_loader"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FO6fBrxZgO6j","colab_type":"code","outputId":"6b636d37-cdd2-450e-ad93-6a1d0051de77","executionInfo":{"status":"ok","timestamp":1590894629347,"user_tz":420,"elapsed":346,"user":{"displayName":"Frances Yu","photoUrl":"","userId":"14628227363021680713"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["test_text = range(50)\n","t_loader = batch_data(test_text, sequence_length=5, batch_size=2)\n","\n","data_iter = iter(t_loader)\n","sample_x, sample_y = data_iter.next()\n","\n","print(sample_x.shape)\n","print(sample_x)\n","print()\n","print(sample_y.shape)\n","print(sample_y)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["torch.Size([2, 5])\n","tensor([[32, 33, 34, 35, 36],\n","        [40, 41, 42, 43, 44]])\n","\n","torch.Size([2])\n","tensor([37, 45])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NcVKegk7gO6q","colab_type":"text"},"source":["# 4. Build the Neural Network"]},{"cell_type":"code","metadata":{"id":"GorXQeoBgO6r","colab_type":"code","colab":{}},"source":["class RNN(nn.Module):\n","    \n","    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):\n","        \"\"\"\n","        Initialize the PyTorch RNN Module\n","        :param vocab_size: The number of input dimensions of the neural network (the size of the vocabulary)\n","        :param output_size: The number of output dimensions of the neural network\n","        :param embedding_dim: The size of embeddings, should you choose to use them        \n","        :param hidden_dim: The size of the hidden layer outputs\n","        :param dropout: dropout to add in between LSTM/GRU layers\n","        \"\"\"\n","        super(RNN, self).__init__()\n","        \n","        # set class variables\n","        self.input_dim = vocab_size\n","        self.output_dim = output_size\n","        self.n_hidden = hidden_dim\n","        self.embedding_dim = embedding_dim\n","        self.n_layers = n_layers\n","        \n","        # define model layers\n","        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm = nn.LSTM(\n","            embedding_dim, \n","            hidden_dim, \n","            batch_first=True, \n","            num_layers=n_layers,\n","            dropout=dropout)\n","        self.linear = nn.Linear(hidden_dim, output_size)\n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def init_hidden(self, batch_size):\n","        ''' Initializes hidden state '''\n","        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n","        # initialized to zero, for hidden state and cell state of LSTM\n","        weight = next(self.parameters()).data\n","        \n","        if (train_on_gpu):\n","            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n","                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n","        else:\n","            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n","                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n","        \n","        return hidden\n","    \n","    def forward(self, nn_input, hidden):\n","        \"\"\"\n","        Forward propagation of the neural network\n","        :param nn_input: The input to the neural network\n","        :param hidden: The hidden state        \n","        :return: Two Tensors, the output of the neural network and the latest hidden state\n","        \"\"\"\n","        \n","        batch_size = nn_input.size(0)\n","        \n","        embeds = self.embeddings(nn_input)\n","        \n","        lstm_out, hidden = self.lstm(embeds, hidden)\n","        \n","        lstm_out = self.dropout(lstm_out)\n","        \n","        lstm_out = lstm_out.contiguous().view(-1, self.n_hidden)\n","        \n","        output = self.linear(lstm_out)\n","        \n","        output = output.view(batch_size, -1, self.output_dim)\n","        \n","        out = output[:, -1]\n","        \n","        return out, hidden"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PY5xiWrJgO6y","colab_type":"code","outputId":"44fea252-2fbf-43df-b3e7-855ca97b43f5","colab":{}},"source":["tests.test_rnn(RNN, train_on_gpu)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Tests Passed\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"38jN5u20gO64","colab_type":"text"},"source":["### Define forward and backpropagation"]},{"cell_type":"code","metadata":{"id":"OaIsdM4lgO66","colab_type":"code","colab":{}},"source":["def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):\n","    \"\"\"\n","    Forward and backward propagation on the neural network\n","    :param rnn: The PyTorch Module that holds the neural network\n","    :param optimizer: The PyTorch optimizer for the neural network\n","    :param criterion: The PyTorch loss function\n","    :param inp: A batch of input to the neural network\n","    :param target: The target output for the batch of input\n","    :return: The loss and the latest hidden state Tensor\n","    \"\"\"\n","    \n","    if train_on_gpu:\n","        inp, target = inp.cuda(), target.cuda()\n","\n","    # Creating new variables for the hidden state, otherwise\n","    # we'd backprop through the entire training history\n","    hidden = tuple([each.data for each in hidden])\n","    \n","    # zero accumulated gradients\n","    rnn.zero_grad()\n","    \n","    # get the output from the model\n","    output, hidden = rnn(inp, hidden)\n","    \n","    # calculate the loss and perform backprop\n","    loss = criterion(output, target)\n","    loss.backward()\n","     \n","    optimizer.step()\n","    \n","    return loss.item(), hidden"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PzBxFizHgO6_","colab_type":"code","outputId":"d60ae0eb-48f3-4f6d-e592-d31e10d2e706","colab":{}},"source":["tests.test_forward_back_prop(RNN, forward_back_prop, train_on_gpu)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Tests Passed\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"R4eWESkZgO7F","colab_type":"text"},"source":["# 5. Neural Network Training"]},{"cell_type":"markdown","metadata":{"id":"VE2iCyuPgO7H","colab_type":"text"},"source":["### Train Loop"]},{"cell_type":"code","metadata":{"id":"nH4wn49ugO7J","colab_type":"code","colab":{}},"source":["def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):\n","    batch_losses = []\n","    \n","    rnn.train()\n","\n","    print(\"Training for %d epoch(s)...\" % n_epochs)\n","    for epoch_i in range(1, n_epochs + 1):\n","        \n","        # initialize hidden state\n","        hidden = rnn.init_hidden(batch_size)\n","        \n","        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n","            \n","            # make sure you iterate over completely full batches, only\n","            n_batches = len(train_loader.dataset)//batch_size\n","            if(batch_i > n_batches):\n","                break\n","            \n","            # forward, back prop\n","            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)          \n","            # record loss\n","            batch_losses.append(loss)\n","\n","            # printing loss stats\n","            if batch_i % show_every_n_batches == 0:\n","                print('Epoch: {:>4}/{:<4}  Loss: {}\\n'.format(\n","                    epoch_i, n_epochs, np.average(batch_losses)))\n","                batch_losses = []\n","\n","    # returns a trained rnn\n","    return rnn"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0_HwJ90xgO7N","colab_type":"code","colab":{}},"source":["# Data params\n","# Sequence Length\n","sequence_length = 20  # of words in a sequence\n","# Batch Size\n","batch_size = 128\n","\n","# data loader - do not change\n","train_loader = batch_data(int_text, sequence_length, batch_size)\n","\n","# Training parameters\n","# Number of Epochs\n","num_epochs = 50\n","# Learning Rate\n","learning_rate = 0.0005\n","\n","# Model parameters\n","# Vocab size\n","vocab_size = len(vocab_to_int)\n","# Output size\n","output_size = vocab_size\n","# Embedding Dimension\n","embedding_dim = 250\n","# Hidden Dimension\n","hidden_dim = 256\n","# Number of RNN Layers\n","n_layers = 2\n","\n","# Show stats for every n number of batches\n","show_every_n_batches = 500"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Icex5HKgO7X","colab_type":"text"},"source":["### Train"]},{"cell_type":"code","metadata":{"id":"ZffH_HvygO7Z","colab_type":"code","outputId":"e0dd8bb5-7127-43c1-a9df-1ecc7535cabb","executionInfo":{"status":"ok","timestamp":1590902959761,"user_tz":420,"elapsed":8311864,"user":{"displayName":"Frances Yu","photoUrl":"","userId":"14628227363021680713"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5)\n","if train_on_gpu:\n","    rnn.cuda()\n","\n","# defining loss and optimization functions for training\n","optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n","criterion = nn.CrossEntropyLoss()\n","\n","# training the model\n","trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches)\n","\n","# saving the trained model\n","save_model('gdrive/My Drive/Lab/save/trained_rnn_2.pt', trained_rnn)\n","print('Model Trained and Saved')"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Training for 50 epoch(s)...\n","Epoch:    1/50    Loss: 5.727044672012329\n","\n","Epoch:    1/50    Loss: 5.096570730209351\n","\n","Epoch:    1/50    Loss: 4.927709865570068\n","\n","Epoch:    1/50    Loss: 4.813870759963989\n","\n","Epoch:    1/50    Loss: 4.710942232131958\n","\n","Epoch:    1/50    Loss: 4.64938889503479\n","\n","Epoch:    1/50    Loss: 4.599518339633942\n","\n","Epoch:    1/50    Loss: 4.549861306667328\n","\n","Epoch:    1/50    Loss: 4.527868047714233\n","\n","Epoch:    1/50    Loss: 4.483501790046692\n","\n","Epoch:    1/50    Loss: 4.462475244045257\n","\n","Epoch:    1/50    Loss: 4.421815739154816\n","\n","Epoch:    1/50    Loss: 4.420301042079926\n","\n","Epoch:    2/50    Loss: 4.326593027870648\n","\n","Epoch:    2/50    Loss: 4.3114853210449215\n","\n","Epoch:    2/50    Loss: 4.296925078392029\n","\n","Epoch:    2/50    Loss: 4.300735924243927\n","\n","Epoch:    2/50    Loss: 4.25222027683258\n","\n","Epoch:    2/50    Loss: 4.2364169049263\n","\n","Epoch:    2/50    Loss: 4.2542314162254335\n","\n","Epoch:    2/50    Loss: 4.242698550224304\n","\n","Epoch:    2/50    Loss: 4.250255524635315\n","\n","Epoch:    2/50    Loss: 4.236949360370636\n","\n","Epoch:    2/50    Loss: 4.242836932182312\n","\n","Epoch:    2/50    Loss: 4.212246623516083\n","\n","Epoch:    2/50    Loss: 4.218161010265351\n","\n","Epoch:    3/50    Loss: 4.185550967147875\n","\n","Epoch:    3/50    Loss: 4.134097635746002\n","\n","Epoch:    3/50    Loss: 4.129531848430633\n","\n","Epoch:    3/50    Loss: 4.1089022040367125\n","\n","Epoch:    3/50    Loss: 4.12807400560379\n","\n","Epoch:    3/50    Loss: 4.128797486782074\n","\n","Epoch:    3/50    Loss: 4.139233131885528\n","\n","Epoch:    3/50    Loss: 4.124909157752991\n","\n","Epoch:    3/50    Loss: 4.1028013730049135\n","\n","Epoch:    3/50    Loss: 4.129899280071259\n","\n","Epoch:    3/50    Loss: 4.097004997253418\n","\n","Epoch:    3/50    Loss: 4.107757843971252\n","\n","Epoch:    3/50    Loss: 4.10876277923584\n","\n","Epoch:    4/50    Loss: 4.0739831792674295\n","\n","Epoch:    4/50    Loss: 4.039386620521546\n","\n","Epoch:    4/50    Loss: 4.047886300563812\n","\n","Epoch:    4/50    Loss: 4.0342856564521785\n","\n","Epoch:    4/50    Loss: 4.0513130493164065\n","\n","Epoch:    4/50    Loss: 4.0480325922966\n","\n","Epoch:    4/50    Loss: 4.030347185134888\n","\n","Epoch:    4/50    Loss: 4.036152066230774\n","\n","Epoch:    4/50    Loss: 4.025736987113953\n","\n","Epoch:    4/50    Loss: 4.045212448120117\n","\n","Epoch:    4/50    Loss: 4.057159357070923\n","\n","Epoch:    4/50    Loss: 4.0285574932098385\n","\n","Epoch:    4/50    Loss: 4.044750220298767\n","\n","Epoch:    5/50    Loss: 4.011879850353762\n","\n","Epoch:    5/50    Loss: 3.9734310669898987\n","\n","Epoch:    5/50    Loss: 3.983695753097534\n","\n","Epoch:    5/50    Loss: 3.9736089072227476\n","\n","Epoch:    5/50    Loss: 3.9782173709869384\n","\n","Epoch:    5/50    Loss: 3.9711142950057985\n","\n","Epoch:    5/50    Loss: 4.000471105575562\n","\n","Epoch:    5/50    Loss: 3.998899329662323\n","\n","Epoch:    5/50    Loss: 3.9907244844436645\n","\n","Epoch:    5/50    Loss: 3.9886384015083314\n","\n","Epoch:    5/50    Loss: 3.9985810022354125\n","\n","Epoch:    5/50    Loss: 4.001355764865875\n","\n","Epoch:    5/50    Loss: 3.98248731470108\n","\n","Epoch:    6/50    Loss: 3.9744204127378335\n","\n","Epoch:    6/50    Loss: 3.9335986166000367\n","\n","Epoch:    6/50    Loss: 3.9266280636787414\n","\n","Epoch:    6/50    Loss: 3.9346275172233582\n","\n","Epoch:    6/50    Loss: 3.934032591342926\n","\n","Epoch:    6/50    Loss: 3.952664958000183\n","\n","Epoch:    6/50    Loss: 3.9492372765541077\n","\n","Epoch:    6/50    Loss: 3.940150918483734\n","\n","Epoch:    6/50    Loss: 3.9358045949935914\n","\n","Epoch:    6/50    Loss: 3.9422288942337036\n","\n","Epoch:    6/50    Loss: 3.950449362277985\n","\n","Epoch:    6/50    Loss: 3.9525371170043946\n","\n","Epoch:    6/50    Loss: 3.9754197812080383\n","\n","Epoch:    7/50    Loss: 3.9123529062778286\n","\n","Epoch:    7/50    Loss: 3.880757520675659\n","\n","Epoch:    7/50    Loss: 3.9055033311843874\n","\n","Epoch:    7/50    Loss: 3.888562017917633\n","\n","Epoch:    7/50    Loss: 3.888986226081848\n","\n","Epoch:    7/50    Loss: 3.912407555103302\n","\n","Epoch:    7/50    Loss: 3.920801420211792\n","\n","Epoch:    7/50    Loss: 3.917532565116882\n","\n","Epoch:    7/50    Loss: 3.9144569191932677\n","\n","Epoch:    7/50    Loss: 3.938617046833038\n","\n","Epoch:    7/50    Loss: 3.9228194971084593\n","\n","Epoch:    7/50    Loss: 3.919901782989502\n","\n","Epoch:    7/50    Loss: 3.919055205345154\n","\n","Epoch:    8/50    Loss: 3.9005458133685575\n","\n","Epoch:    8/50    Loss: 3.840033977508545\n","\n","Epoch:    8/50    Loss: 3.8750543417930605\n","\n","Epoch:    8/50    Loss: 3.8664033069610597\n","\n","Epoch:    8/50    Loss: 3.862764886856079\n","\n","Epoch:    8/50    Loss: 3.878661128997803\n","\n","Epoch:    8/50    Loss: 3.865588452339172\n","\n","Epoch:    8/50    Loss: 3.8888025612831116\n","\n","Epoch:    8/50    Loss: 3.866827826976776\n","\n","Epoch:    8/50    Loss: 3.8910589423179625\n","\n","Epoch:    8/50    Loss: 3.9166402983665467\n","\n","Epoch:    8/50    Loss: 3.9125037741661073\n","\n","Epoch:    8/50    Loss: 3.896125865459442\n","\n","Epoch:    9/50    Loss: 3.8677585246789192\n","\n","Epoch:    9/50    Loss: 3.836387116909027\n","\n","Epoch:    9/50    Loss: 3.8276024141311646\n","\n","Epoch:    9/50    Loss: 3.830982309818268\n","\n","Epoch:    9/50    Loss: 3.851409091472626\n","\n","Epoch:    9/50    Loss: 3.861551987171173\n","\n","Epoch:    9/50    Loss: 3.859476951599121\n","\n","Epoch:    9/50    Loss: 3.8641923966407776\n","\n","Epoch:    9/50    Loss: 3.8412530999183656\n","\n","Epoch:    9/50    Loss: 3.8599182934761047\n","\n","Epoch:    9/50    Loss: 3.885923595905304\n","\n","Epoch:    9/50    Loss: 3.85805073928833\n","\n","Epoch:    9/50    Loss: 3.890541762828827\n","\n","Epoch:   10/50    Loss: 3.837193145244785\n","\n","Epoch:   10/50    Loss: 3.7981142950057984\n","\n","Epoch:   10/50    Loss: 3.8145757546424868\n","\n","Epoch:   10/50    Loss: 3.819693861961365\n","\n","Epoch:   10/50    Loss: 3.8499727444648744\n","\n","Epoch:   10/50    Loss: 3.8176704607009886\n","\n","Epoch:   10/50    Loss: 3.8123094530105592\n","\n","Epoch:   10/50    Loss: 3.8271615948677065\n","\n","Epoch:   10/50    Loss: 3.8665096197128297\n","\n","Epoch:   10/50    Loss: 3.8526707859039306\n","\n","Epoch:   10/50    Loss: 3.819319637298584\n","\n","Epoch:   10/50    Loss: 3.8519350996017456\n","\n","Epoch:   10/50    Loss: 3.8410566248893736\n","\n","Epoch:   11/50    Loss: 3.818321225541227\n","\n","Epoch:   11/50    Loss: 3.7751289410591125\n","\n","Epoch:   11/50    Loss: 3.785886424541473\n","\n","Epoch:   11/50    Loss: 3.795367452144623\n","\n","Epoch:   11/50    Loss: 3.779746021747589\n","\n","Epoch:   11/50    Loss: 3.789408847332001\n","\n","Epoch:   11/50    Loss: 3.817147162914276\n","\n","Epoch:   11/50    Loss: 3.8145255126953126\n","\n","Epoch:   11/50    Loss: 3.842538369178772\n","\n","Epoch:   11/50    Loss: 3.827753683567047\n","\n","Epoch:   11/50    Loss: 3.8427363004684447\n","\n","Epoch:   11/50    Loss: 3.8278241224288942\n","\n","Epoch:   11/50    Loss: 3.8361005759239197\n","\n","Epoch:   12/50    Loss: 3.7882507585758214\n","\n","Epoch:   12/50    Loss: 3.7598951783180237\n","\n","Epoch:   12/50    Loss: 3.7931886405944826\n","\n","Epoch:   12/50    Loss: 3.7858346037864683\n","\n","Epoch:   12/50    Loss: 3.769317047595978\n","\n","Epoch:   12/50    Loss: 3.7863082509040833\n","\n","Epoch:   12/50    Loss: 3.7733642220497132\n","\n","Epoch:   12/50    Loss: 3.789328466415405\n","\n","Epoch:   12/50    Loss: 3.7920554146766663\n","\n","Epoch:   12/50    Loss: 3.810022729873657\n","\n","Epoch:   12/50    Loss: 3.829050957202911\n","\n","Epoch:   12/50    Loss: 3.8072768173217773\n","\n","Epoch:   12/50    Loss: 3.822666221618652\n","\n","Epoch:   13/50    Loss: 3.7780467723034965\n","\n","Epoch:   13/50    Loss: 3.7445288109779358\n","\n","Epoch:   13/50    Loss: 3.7382938203811644\n","\n","Epoch:   13/50    Loss: 3.7506570358276368\n","\n","Epoch:   13/50    Loss: 3.776544030666351\n","\n","Epoch:   13/50    Loss: 3.751832482814789\n","\n","Epoch:   13/50    Loss: 3.7706248354911804\n","\n","Epoch:   13/50    Loss: 3.791032284259796\n","\n","Epoch:   13/50    Loss: 3.7597008538246155\n","\n","Epoch:   13/50    Loss: 3.801545159339905\n","\n","Epoch:   13/50    Loss: 3.7808991141319277\n","\n","Epoch:   13/50    Loss: 3.804444043636322\n","\n","Epoch:   13/50    Loss: 3.7997621393203733\n","\n","Epoch:   14/50    Loss: 3.7558433693815196\n","\n","Epoch:   14/50    Loss: 3.71620344209671\n","\n","Epoch:   14/50    Loss: 3.723082858085632\n","\n","Epoch:   14/50    Loss: 3.7446769132614137\n","\n","Epoch:   14/50    Loss: 3.7471253209114073\n","\n","Epoch:   14/50    Loss: 3.7778517198562622\n","\n","Epoch:   14/50    Loss: 3.7473320989608765\n","\n","Epoch:   14/50    Loss: 3.7584207611083986\n","\n","Epoch:   14/50    Loss: 3.7512504196166994\n","\n","Epoch:   14/50    Loss: 3.7630239682197573\n","\n","Epoch:   14/50    Loss: 3.7946255497932433\n","\n","Epoch:   14/50    Loss: 3.7939152989387512\n","\n","Epoch:   14/50    Loss: 3.7667585673332216\n","\n","Epoch:   15/50    Loss: 3.7506173340694997\n","\n","Epoch:   15/50    Loss: 3.7366498608589174\n","\n","Epoch:   15/50    Loss: 3.7102990899085997\n","\n","Epoch:   15/50    Loss: 3.7380457048416136\n","\n","Epoch:   15/50    Loss: 3.740068524837494\n","\n","Epoch:   15/50    Loss: 3.726727818965912\n","\n","Epoch:   15/50    Loss: 3.7418424730300903\n","\n","Epoch:   15/50    Loss: 3.729959470748901\n","\n","Epoch:   15/50    Loss: 3.741971389770508\n","\n","Epoch:   15/50    Loss: 3.765554032802582\n","\n","Epoch:   15/50    Loss: 3.7595600271224976\n","\n","Epoch:   15/50    Loss: 3.7323511958122255\n","\n","Epoch:   15/50    Loss: 3.740713526725769\n","\n","Epoch:   16/50    Loss: 3.7192963810484154\n","\n","Epoch:   16/50    Loss: 3.694264033317566\n","\n","Epoch:   16/50    Loss: 3.7129513869285584\n","\n","Epoch:   16/50    Loss: 3.720483166694641\n","\n","Epoch:   16/50    Loss: 3.7006043701171873\n","\n","Epoch:   16/50    Loss: 3.7178235883712767\n","\n","Epoch:   16/50    Loss: 3.7181396894454957\n","\n","Epoch:   16/50    Loss: 3.722462135314941\n","\n","Epoch:   16/50    Loss: 3.7501013650894164\n","\n","Epoch:   16/50    Loss: 3.735883846282959\n","\n","Epoch:   16/50    Loss: 3.7369771394729616\n","\n","Epoch:   16/50    Loss: 3.734067537307739\n","\n","Epoch:   16/50    Loss: 3.7367953672409056\n","\n","Epoch:   17/50    Loss: 3.7072802422814872\n","\n","Epoch:   17/50    Loss: 3.6969428434371947\n","\n","Epoch:   17/50    Loss: 3.684756284713745\n","\n","Epoch:   17/50    Loss: 3.6976626353263855\n","\n","Epoch:   17/50    Loss: 3.708182616710663\n","\n","Epoch:   17/50    Loss: 3.695426197528839\n","\n","Epoch:   17/50    Loss: 3.735405483722687\n","\n","Epoch:   17/50    Loss: 3.6994270987510682\n","\n","Epoch:   17/50    Loss: 3.729401861667633\n","\n","Epoch:   17/50    Loss: 3.716475526332855\n","\n","Epoch:   17/50    Loss: 3.718295958995819\n","\n","Epoch:   17/50    Loss: 3.7152640953063965\n","\n","Epoch:   17/50    Loss: 3.7366717195510866\n","\n","Epoch:   18/50    Loss: 3.6902851081366834\n","\n","Epoch:   18/50    Loss: 3.675288435459137\n","\n","Epoch:   18/50    Loss: 3.664470621109009\n","\n","Epoch:   18/50    Loss: 3.664560537815094\n","\n","Epoch:   18/50    Loss: 3.6878924760818483\n","\n","Epoch:   18/50    Loss: 3.6880684032440185\n","\n","Epoch:   18/50    Loss: 3.7054259152412414\n","\n","Epoch:   18/50    Loss: 3.6858075127601624\n","\n","Epoch:   18/50    Loss: 3.7313922681808473\n","\n","Epoch:   18/50    Loss: 3.730036310195923\n","\n","Epoch:   18/50    Loss: 3.686372886657715\n","\n","Epoch:   18/50    Loss: 3.7099843883514403\n","\n","Epoch:   18/50    Loss: 3.7189503245353697\n","\n","Epoch:   19/50    Loss: 3.673673573574508\n","\n","Epoch:   19/50    Loss: 3.6577029566764834\n","\n","Epoch:   19/50    Loss: 3.6798735704422\n","\n","Epoch:   19/50    Loss: 3.6728892822265626\n","\n","Epoch:   19/50    Loss: 3.673183678150177\n","\n","Epoch:   19/50    Loss: 3.6946906909942627\n","\n","Epoch:   19/50    Loss: 3.672423840045929\n","\n","Epoch:   19/50    Loss: 3.6901963300704956\n","\n","Epoch:   19/50    Loss: 3.707542321205139\n","\n","Epoch:   19/50    Loss: 3.69847776889801\n","\n","Epoch:   19/50    Loss: 3.7050313901901246\n","\n","Epoch:   19/50    Loss: 3.7024758281707766\n","\n","Epoch:   19/50    Loss: 3.684155438423157\n","\n","Epoch:   20/50    Loss: 3.68083684412108\n","\n","Epoch:   20/50    Loss: 3.626953103542328\n","\n","Epoch:   20/50    Loss: 3.664771576881409\n","\n","Epoch:   20/50    Loss: 3.6577345070838927\n","\n","Epoch:   20/50    Loss: 3.6552504959106447\n","\n","Epoch:   20/50    Loss: 3.6592567420005797\n","\n","Epoch:   20/50    Loss: 3.6717973990440367\n","\n","Epoch:   20/50    Loss: 3.6773970918655396\n","\n","Epoch:   20/50    Loss: 3.6630893197059633\n","\n","Epoch:   20/50    Loss: 3.6935876626968382\n","\n","Epoch:   20/50    Loss: 3.692822196483612\n","\n","Epoch:   20/50    Loss: 3.683922734737396\n","\n","Epoch:   20/50    Loss: 3.691083456039429\n","\n","Epoch:   21/50    Loss: 3.6466979348995143\n","\n","Epoch:   21/50    Loss: 3.6114775018692016\n","\n","Epoch:   21/50    Loss: 3.6368687987327575\n","\n","Epoch:   21/50    Loss: 3.643064859390259\n","\n","Epoch:   21/50    Loss: 3.660503534793854\n","\n","Epoch:   21/50    Loss: 3.667271601676941\n","\n","Epoch:   21/50    Loss: 3.6676547808647157\n","\n","Epoch:   21/50    Loss: 3.655974662780762\n","\n","Epoch:   21/50    Loss: 3.6628579955101013\n","\n","Epoch:   21/50    Loss: 3.670722153663635\n","\n","Epoch:   21/50    Loss: 3.702303686618805\n","\n","Epoch:   21/50    Loss: 3.6669403977394106\n","\n","Epoch:   21/50    Loss: 3.6672170042991636\n","\n","Epoch:   22/50    Loss: 3.646407570406344\n","\n","Epoch:   22/50    Loss: 3.6301836514472963\n","\n","Epoch:   22/50    Loss: 3.63968896818161\n","\n","Epoch:   22/50    Loss: 3.6304447269439697\n","\n","Epoch:   22/50    Loss: 3.6224839930534363\n","\n","Epoch:   22/50    Loss: 3.629790342330933\n","\n","Epoch:   22/50    Loss: 3.6442961525917053\n","\n","Epoch:   22/50    Loss: 3.654866714000702\n","\n","Epoch:   22/50    Loss: 3.665324324607849\n","\n","Epoch:   22/50    Loss: 3.650426875114441\n","\n","Epoch:   22/50    Loss: 3.6631689047813416\n","\n","Epoch:   22/50    Loss: 3.6697912015914915\n","\n","Epoch:   22/50    Loss: 3.6734978709220885\n","\n","Epoch:   23/50    Loss: 3.632414833223981\n","\n","Epoch:   23/50    Loss: 3.6159342083930968\n","\n","Epoch:   23/50    Loss: 3.594747784614563\n","\n","Epoch:   23/50    Loss: 3.637513078212738\n","\n","Epoch:   23/50    Loss: 3.6386864500045775\n","\n","Epoch:   23/50    Loss: 3.623960165977478\n","\n","Epoch:   23/50    Loss: 3.6243889827728273\n","\n","Epoch:   23/50    Loss: 3.646763847351074\n","\n","Epoch:   23/50    Loss: 3.62939377784729\n","\n","Epoch:   23/50    Loss: 3.6342012367248535\n","\n","Epoch:   23/50    Loss: 3.6517461409568788\n","\n","Epoch:   23/50    Loss: 3.6504628949165343\n","\n","Epoch:   23/50    Loss: 3.6671452016830446\n","\n","Epoch:   24/50    Loss: 3.6435222762468835\n","\n","Epoch:   24/50    Loss: 3.569810954093933\n","\n","Epoch:   24/50    Loss: 3.6085071606636046\n","\n","Epoch:   24/50    Loss: 3.63210658788681\n","\n","Epoch:   24/50    Loss: 3.6090157995223997\n","\n","Epoch:   24/50    Loss: 3.625146541118622\n","\n","Epoch:   24/50    Loss: 3.6190049624443055\n","\n","Epoch:   24/50    Loss: 3.640250774383545\n","\n","Epoch:   24/50    Loss: 3.6359838533401487\n","\n","Epoch:   24/50    Loss: 3.6210694065093993\n","\n","Epoch:   24/50    Loss: 3.6570776081085206\n","\n","Epoch:   24/50    Loss: 3.6247358179092406\n","\n","Epoch:   24/50    Loss: 3.657334429264069\n","\n","Epoch:   25/50    Loss: 3.60900391602541\n","\n","Epoch:   25/50    Loss: 3.5545593876838684\n","\n","Epoch:   25/50    Loss: 3.5871699476242065\n","\n","Epoch:   25/50    Loss: 3.621353545665741\n","\n","Epoch:   25/50    Loss: 3.608180956363678\n","\n","Epoch:   25/50    Loss: 3.626446799278259\n","\n","Epoch:   25/50    Loss: 3.5916578121185303\n","\n","Epoch:   25/50    Loss: 3.6274023871421814\n","\n","Epoch:   25/50    Loss: 3.622539743423462\n","\n","Epoch:   25/50    Loss: 3.631285620689392\n","\n","Epoch:   25/50    Loss: 3.6483923811912535\n","\n","Epoch:   25/50    Loss: 3.6307330646514893\n","\n","Epoch:   25/50    Loss: 3.6568566770553588\n","\n","Epoch:   26/50    Loss: 3.599242886363279\n","\n","Epoch:   26/50    Loss: 3.5776256575584413\n","\n","Epoch:   26/50    Loss: 3.587185025691986\n","\n","Epoch:   26/50    Loss: 3.592562349796295\n","\n","Epoch:   26/50    Loss: 3.5983707704544066\n","\n","Epoch:   26/50    Loss: 3.5907462735176088\n","\n","Epoch:   26/50    Loss: 3.631590017795563\n","\n","Epoch:   26/50    Loss: 3.6176048827171328\n","\n","Epoch:   26/50    Loss: 3.597070861339569\n","\n","Epoch:   26/50    Loss: 3.619211683273315\n","\n","Epoch:   26/50    Loss: 3.614399353981018\n","\n","Epoch:   26/50    Loss: 3.6391277294158937\n","\n","Epoch:   26/50    Loss: 3.6297074909210205\n","\n","Epoch:   27/50    Loss: 3.595120074478006\n","\n","Epoch:   27/50    Loss: 3.585863627910614\n","\n","Epoch:   27/50    Loss: 3.58955092048645\n","\n","Epoch:   27/50    Loss: 3.5693370685577395\n","\n","Epoch:   27/50    Loss: 3.606161340713501\n","\n","Epoch:   27/50    Loss: 3.6046078848838805\n","\n","Epoch:   27/50    Loss: 3.5845057001113894\n","\n","Epoch:   27/50    Loss: 3.6021736936569213\n","\n","Epoch:   27/50    Loss: 3.615481686115265\n","\n","Epoch:   27/50    Loss: 3.604218970298767\n","\n","Epoch:   27/50    Loss: 3.622973714828491\n","\n","Epoch:   27/50    Loss: 3.596690477848053\n","\n","Epoch:   27/50    Loss: 3.621539240837097\n","\n","Epoch:   28/50    Loss: 3.589487619519358\n","\n","Epoch:   28/50    Loss: 3.5571944708824157\n","\n","Epoch:   28/50    Loss: 3.578822097301483\n","\n","Epoch:   28/50    Loss: 3.6037328586578368\n","\n","Epoch:   28/50    Loss: 3.5719371485710143\n","\n","Epoch:   28/50    Loss: 3.592937028884888\n","\n","Epoch:   28/50    Loss: 3.5799015789031983\n","\n","Epoch:   28/50    Loss: 3.614033190250397\n","\n","Epoch:   28/50    Loss: 3.594585482120514\n","\n","Epoch:   28/50    Loss: 3.616928792476654\n","\n","Epoch:   28/50    Loss: 3.6085873470306398\n","\n","Epoch:   28/50    Loss: 3.586462319850922\n","\n","Epoch:   28/50    Loss: 3.6068349471092223\n","\n","Epoch:   29/50    Loss: 3.578687043334197\n","\n","Epoch:   29/50    Loss: 3.541213304042816\n","\n","Epoch:   29/50    Loss: 3.564715610027313\n","\n","Epoch:   29/50    Loss: 3.557080515384674\n","\n","Epoch:   29/50    Loss: 3.561963402271271\n","\n","Epoch:   29/50    Loss: 3.5792857398986815\n","\n","Epoch:   29/50    Loss: 3.5902033972740175\n","\n","Epoch:   29/50    Loss: 3.6055830092430114\n","\n","Epoch:   29/50    Loss: 3.5978331747055052\n","\n","Epoch:   29/50    Loss: 3.579730652332306\n","\n","Epoch:   29/50    Loss: 3.58127965927124\n","\n","Epoch:   29/50    Loss: 3.611574101924896\n","\n","Epoch:   29/50    Loss: 3.6069782285690306\n","\n","Epoch:   30/50    Loss: 3.56543498069079\n","\n","Epoch:   30/50    Loss: 3.5326242246627806\n","\n","Epoch:   30/50    Loss: 3.5528975448608398\n","\n","Epoch:   30/50    Loss: 3.547983448982239\n","\n","Epoch:   30/50    Loss: 3.5918719005584716\n","\n","Epoch:   30/50    Loss: 3.5449094672203065\n","\n","Epoch:   30/50    Loss: 3.5609951586723327\n","\n","Epoch:   30/50    Loss: 3.5761502017974856\n","\n","Epoch:   30/50    Loss: 3.5991760816574097\n","\n","Epoch:   30/50    Loss: 3.5919819889068605\n","\n","Epoch:   30/50    Loss: 3.5864205298423766\n","\n","Epoch:   30/50    Loss: 3.5737492289543153\n","\n","Epoch:   30/50    Loss: 3.608231303215027\n","\n","Epoch:   31/50    Loss: 3.574149687421956\n","\n","Epoch:   31/50    Loss: 3.552593759059906\n","\n","Epoch:   31/50    Loss: 3.537800458908081\n","\n","Epoch:   31/50    Loss: 3.549415415763855\n","\n","Epoch:   31/50    Loss: 3.553330120563507\n","\n","Epoch:   31/50    Loss: 3.555519850254059\n","\n","Epoch:   31/50    Loss: 3.566647946357727\n","\n","Epoch:   31/50    Loss: 3.5535573344230653\n","\n","Epoch:   31/50    Loss: 3.575469727039337\n","\n","Epoch:   31/50    Loss: 3.581882737159729\n","\n","Epoch:   31/50    Loss: 3.567487904548645\n","\n","Epoch:   31/50    Loss: 3.5811201977729796\n","\n","Epoch:   31/50    Loss: 3.5960482029914855\n","\n","Epoch:   32/50    Loss: 3.5615897380025343\n","\n","Epoch:   32/50    Loss: 3.525271055698395\n","\n","Epoch:   32/50    Loss: 3.5286706714630127\n","\n","Epoch:   32/50    Loss: 3.5503735933303835\n","\n","Epoch:   32/50    Loss: 3.5393112053871154\n","\n","Epoch:   32/50    Loss: 3.5375133395195006\n","\n","Epoch:   32/50    Loss: 3.553910453796387\n","\n","Epoch:   32/50    Loss: 3.535876084804535\n","\n","Epoch:   32/50    Loss: 3.5682232365608217\n","\n","Epoch:   32/50    Loss: 3.555239677429199\n","\n","Epoch:   32/50    Loss: 3.5688203411102295\n","\n","Epoch:   32/50    Loss: 3.605173802852631\n","\n","Epoch:   32/50    Loss: 3.585849582195282\n","\n","Epoch:   33/50    Loss: 3.5462418552235593\n","\n","Epoch:   33/50    Loss: 3.506323247909546\n","\n","Epoch:   33/50    Loss: 3.5206674313545228\n","\n","Epoch:   33/50    Loss: 3.5321886920928955\n","\n","Epoch:   33/50    Loss: 3.549938103199005\n","\n","Epoch:   33/50    Loss: 3.548279173374176\n","\n","Epoch:   33/50    Loss: 3.5289374833106995\n","\n","Epoch:   33/50    Loss: 3.547023711681366\n","\n","Epoch:   33/50    Loss: 3.5924439458847046\n","\n","Epoch:   33/50    Loss: 3.5615271644592283\n","\n","Epoch:   33/50    Loss: 3.585086371898651\n","\n","Epoch:   33/50    Loss: 3.5700568227767944\n","\n","Epoch:   33/50    Loss: 3.6046791834831238\n","\n","Epoch:   34/50    Loss: 3.543488632526338\n","\n","Epoch:   34/50    Loss: 3.521254997730255\n","\n","Epoch:   34/50    Loss: 3.5316576371192934\n","\n","Epoch:   34/50    Loss: 3.5407167248725893\n","\n","Epoch:   34/50    Loss: 3.5281710138320923\n","\n","Epoch:   34/50    Loss: 3.5481406292915345\n","\n","Epoch:   34/50    Loss: 3.5437434396743774\n","\n","Epoch:   34/50    Loss: 3.5478315782546996\n","\n","Epoch:   34/50    Loss: 3.5411830711364747\n","\n","Epoch:   34/50    Loss: 3.5350889472961424\n","\n","Epoch:   34/50    Loss: 3.54184073972702\n","\n","Epoch:   34/50    Loss: 3.5540732526779175\n","\n","Epoch:   34/50    Loss: 3.5805345883369446\n","\n","Epoch:   35/50    Loss: 3.5334109382410617\n","\n","Epoch:   35/50    Loss: 3.485486708164215\n","\n","Epoch:   35/50    Loss: 3.526979127407074\n","\n","Epoch:   35/50    Loss: 3.549107949256897\n","\n","Epoch:   35/50    Loss: 3.5307750000953675\n","\n","Epoch:   35/50    Loss: 3.5425606331825255\n","\n","Epoch:   35/50    Loss: 3.542331171989441\n","\n","Epoch:   35/50    Loss: 3.527898013114929\n","\n","Epoch:   35/50    Loss: 3.552475179195404\n","\n","Epoch:   35/50    Loss: 3.5622561054229736\n","\n","Epoch:   35/50    Loss: 3.543330986022949\n","\n","Epoch:   35/50    Loss: 3.5308089189529417\n","\n","Epoch:   35/50    Loss: 3.5557449140548707\n","\n","Epoch:   36/50    Loss: 3.519340517125612\n","\n","Epoch:   36/50    Loss: 3.482627662181854\n","\n","Epoch:   36/50    Loss: 3.502039090156555\n","\n","Epoch:   36/50    Loss: 3.5218752241134643\n","\n","Epoch:   36/50    Loss: 3.5182197895050047\n","\n","Epoch:   36/50    Loss: 3.52312184047699\n","\n","Epoch:   36/50    Loss: 3.547250069618225\n","\n","Epoch:   36/50    Loss: 3.5152974581718444\n","\n","Epoch:   36/50    Loss: 3.551592496395111\n","\n","Epoch:   36/50    Loss: 3.55420023727417\n","\n","Epoch:   36/50    Loss: 3.535876628398895\n","\n","Epoch:   36/50    Loss: 3.5653211970329286\n","\n","Epoch:   36/50    Loss: 3.560079023838043\n","\n","Epoch:   37/50    Loss: 3.51119961579475\n","\n","Epoch:   37/50    Loss: 3.484501657962799\n","\n","Epoch:   37/50    Loss: 3.510416127681732\n","\n","Epoch:   37/50    Loss: 3.507659851074219\n","\n","Epoch:   37/50    Loss: 3.5022115015983584\n","\n","Epoch:   37/50    Loss: 3.5046545090675356\n","\n","Epoch:   37/50    Loss: 3.5445500712394713\n","\n","Epoch:   37/50    Loss: 3.5263551473617554\n","\n","Epoch:   37/50    Loss: 3.5320429282188415\n","\n","Epoch:   37/50    Loss: 3.54772882604599\n","\n","Epoch:   37/50    Loss: 3.5661407794952393\n","\n","Epoch:   37/50    Loss: 3.5398868408203126\n","\n","Epoch:   37/50    Loss: 3.543073160648346\n","\n","Epoch:   38/50    Loss: 3.5159611264408817\n","\n","Epoch:   38/50    Loss: 3.5218966007232666\n","\n","Epoch:   38/50    Loss: 3.4988255596160887\n","\n","Epoch:   38/50    Loss: 3.4945894961357116\n","\n","Epoch:   38/50    Loss: 3.494154257774353\n","\n","Epoch:   38/50    Loss: 3.5235313448905945\n","\n","Epoch:   38/50    Loss: 3.4956858134269715\n","\n","Epoch:   38/50    Loss: 3.5292257952690123\n","\n","Epoch:   38/50    Loss: 3.5179568977355955\n","\n","Epoch:   38/50    Loss: 3.5174301009178164\n","\n","Epoch:   38/50    Loss: 3.541216555595398\n","\n","Epoch:   38/50    Loss: 3.525197531700134\n","\n","Epoch:   38/50    Loss: 3.526453012943268\n","\n","Epoch:   39/50    Loss: 3.5215757402811856\n","\n","Epoch:   39/50    Loss: 3.488806537628174\n","\n","Epoch:   39/50    Loss: 3.474654836177826\n","\n","Epoch:   39/50    Loss: 3.494224540233612\n","\n","Epoch:   39/50    Loss: 3.509716178417206\n","\n","Epoch:   39/50    Loss: 3.506872036933899\n","\n","Epoch:   39/50    Loss: 3.4950670080184936\n","\n","Epoch:   39/50    Loss: 3.512092197418213\n","\n","Epoch:   39/50    Loss: 3.491263117313385\n","\n","Epoch:   39/50    Loss: 3.534995713710785\n","\n","Epoch:   39/50    Loss: 3.5282488613128664\n","\n","Epoch:   39/50    Loss: 3.547593476295471\n","\n","Epoch:   39/50    Loss: 3.541948993206024\n","\n","Epoch:   40/50    Loss: 3.5004616922332796\n","\n","Epoch:   40/50    Loss: 3.474417630195618\n","\n","Epoch:   40/50    Loss: 3.4705663385391237\n","\n","Epoch:   40/50    Loss: 3.4898620920181274\n","\n","Epoch:   40/50    Loss: 3.507571425437927\n","\n","Epoch:   40/50    Loss: 3.509211981296539\n","\n","Epoch:   40/50    Loss: 3.5086412935256956\n","\n","Epoch:   40/50    Loss: 3.5062851934432984\n","\n","Epoch:   40/50    Loss: 3.5073977489471435\n","\n","Epoch:   40/50    Loss: 3.514565789222717\n","\n","Epoch:   40/50    Loss: 3.50676664018631\n","\n","Epoch:   40/50    Loss: 3.5247517170906066\n","\n","Epoch:   40/50    Loss: 3.5287141885757447\n","\n","Epoch:   41/50    Loss: 3.507764615157349\n","\n","Epoch:   41/50    Loss: 3.440788248538971\n","\n","Epoch:   41/50    Loss: 3.481287899494171\n","\n","Epoch:   41/50    Loss: 3.4907562317848204\n","\n","Epoch:   41/50    Loss: 3.4882101373672487\n","\n","Epoch:   41/50    Loss: 3.495198823928833\n","\n","Epoch:   41/50    Loss: 3.482196135520935\n","\n","Epoch:   41/50    Loss: 3.5174816198349\n","\n","Epoch:   41/50    Loss: 3.505259147644043\n","\n","Epoch:   41/50    Loss: 3.5014480466842652\n","\n","Epoch:   41/50    Loss: 3.514106201171875\n","\n","Epoch:   41/50    Loss: 3.528021330833435\n","\n","Epoch:   41/50    Loss: 3.5314988098144533\n","\n","Epoch:   42/50    Loss: 3.4863817773844827\n","\n","Epoch:   42/50    Loss: 3.446679852962494\n","\n","Epoch:   42/50    Loss: 3.4761096467971804\n","\n","Epoch:   42/50    Loss: 3.4946581931114196\n","\n","Epoch:   42/50    Loss: 3.466251114368439\n","\n","Epoch:   42/50    Loss: 3.4900051536560057\n","\n","Epoch:   42/50    Loss: 3.486067903995514\n","\n","Epoch:   42/50    Loss: 3.4934904985427857\n","\n","Epoch:   42/50    Loss: 3.5012554602622985\n","\n","Epoch:   42/50    Loss: 3.5067681274414064\n","\n","Epoch:   42/50    Loss: 3.4906144342422487\n","\n","Epoch:   42/50    Loss: 3.5094527258872987\n","\n","Epoch:   42/50    Loss: 3.531245858669281\n","\n","Epoch:   43/50    Loss: 3.479622498543096\n","\n","Epoch:   43/50    Loss: 3.4447932653427125\n","\n","Epoch:   43/50    Loss: 3.4665759062767028\n","\n","Epoch:   43/50    Loss: 3.4882140293121338\n","\n","Epoch:   43/50    Loss: 3.4858831400871275\n","\n","Epoch:   43/50    Loss: 3.4768332476615904\n","\n","Epoch:   43/50    Loss: 3.521964735031128\n","\n","Epoch:   43/50    Loss: 3.502642882823944\n","\n","Epoch:   43/50    Loss: 3.47470840549469\n","\n","Epoch:   43/50    Loss: 3.5068826661109926\n","\n","Epoch:   43/50    Loss: 3.497947593688965\n","\n","Epoch:   43/50    Loss: 3.4793932518959045\n","\n","Epoch:   43/50    Loss: 3.513927602291107\n","\n","Epoch:   44/50    Loss: 3.4728957609787225\n","\n","Epoch:   44/50    Loss: 3.4547208824157716\n","\n","Epoch:   44/50    Loss: 3.471010461807251\n","\n","Epoch:   44/50    Loss: 3.4652090754508973\n","\n","Epoch:   44/50    Loss: 3.464566159725189\n","\n","Epoch:   44/50    Loss: 3.4658198285102846\n","\n","Epoch:   44/50    Loss: 3.4914318375587463\n","\n","Epoch:   44/50    Loss: 3.48497762298584\n","\n","Epoch:   44/50    Loss: 3.483610589504242\n","\n","Epoch:   44/50    Loss: 3.4809668126106263\n","\n","Epoch:   44/50    Loss: 3.498038815498352\n","\n","Epoch:   44/50    Loss: 3.5245712571144105\n","\n","Epoch:   44/50    Loss: 3.506153727054596\n","\n","Epoch:   45/50    Loss: 3.476500909445308\n","\n","Epoch:   45/50    Loss: 3.434407066822052\n","\n","Epoch:   45/50    Loss: 3.441390344619751\n","\n","Epoch:   45/50    Loss: 3.4609602303504943\n","\n","Epoch:   45/50    Loss: 3.4590778427124023\n","\n","Epoch:   45/50    Loss: 3.473230215549469\n","\n","Epoch:   45/50    Loss: 3.489957998752594\n","\n","Epoch:   45/50    Loss: 3.4536837186813356\n","\n","Epoch:   45/50    Loss: 3.4949143333435058\n","\n","Epoch:   45/50    Loss: 3.4815395026206972\n","\n","Epoch:   45/50    Loss: 3.504313518047333\n","\n","Epoch:   45/50    Loss: 3.495965160369873\n","\n","Epoch:   45/50    Loss: 3.5036623158454896\n","\n","Epoch:   46/50    Loss: 3.4750728982581336\n","\n","Epoch:   46/50    Loss: 3.4410907588005064\n","\n","Epoch:   46/50    Loss: 3.434142273426056\n","\n","Epoch:   46/50    Loss: 3.454665328502655\n","\n","Epoch:   46/50    Loss: 3.461732644557953\n","\n","Epoch:   46/50    Loss: 3.4681506767272947\n","\n","Epoch:   46/50    Loss: 3.4718853635787963\n","\n","Epoch:   46/50    Loss: 3.458995165348053\n","\n","Epoch:   46/50    Loss: 3.4759851846694945\n","\n","Epoch:   46/50    Loss: 3.488636772632599\n","\n","Epoch:   46/50    Loss: 3.49119264793396\n","\n","Epoch:   46/50    Loss: 3.5026805467605593\n","\n","Epoch:   46/50    Loss: 3.502896646976471\n","\n","Epoch:   47/50    Loss: 3.452791769884923\n","\n","Epoch:   47/50    Loss: 3.450139449119568\n","\n","Epoch:   47/50    Loss: 3.4530791907310485\n","\n","Epoch:   47/50    Loss: 3.4587474608421327\n","\n","Epoch:   47/50    Loss: 3.4426547265052796\n","\n","Epoch:   47/50    Loss: 3.4621675992012024\n","\n","Epoch:   47/50    Loss: 3.4643548817634584\n","\n","Epoch:   47/50    Loss: 3.4449815368652343\n","\n","Epoch:   47/50    Loss: 3.480188286304474\n","\n","Epoch:   47/50    Loss: 3.4610139365196226\n","\n","Epoch:   47/50    Loss: 3.4618202204704285\n","\n","Epoch:   47/50    Loss: 3.4936055908203123\n","\n","Epoch:   47/50    Loss: 3.501881271839142\n","\n","Epoch:   48/50    Loss: 3.4509172228255287\n","\n","Epoch:   48/50    Loss: 3.432879813194275\n","\n","Epoch:   48/50    Loss: 3.4534969658851624\n","\n","Epoch:   48/50    Loss: 3.43548983669281\n","\n","Epoch:   48/50    Loss: 3.435442554950714\n","\n","Epoch:   48/50    Loss: 3.4850268487930296\n","\n","Epoch:   48/50    Loss: 3.449389982700348\n","\n","Epoch:   48/50    Loss: 3.462853976249695\n","\n","Epoch:   48/50    Loss: 3.4659614052772523\n","\n","Epoch:   48/50    Loss: 3.473676007270813\n","\n","Epoch:   48/50    Loss: 3.4745785360336305\n","\n","Epoch:   48/50    Loss: 3.496016915798187\n","\n","Epoch:   48/50    Loss: 3.500276945590973\n","\n","Epoch:   49/50    Loss: 3.439313656346517\n","\n","Epoch:   49/50    Loss: 3.4253807344436646\n","\n","Epoch:   49/50    Loss: 3.4214488916397094\n","\n","Epoch:   49/50    Loss: 3.4303928351402284\n","\n","Epoch:   49/50    Loss: 3.430305492401123\n","\n","Epoch:   49/50    Loss: 3.447998480796814\n","\n","Epoch:   49/50    Loss: 3.4535883431434633\n","\n","Epoch:   49/50    Loss: 3.462167149066925\n","\n","Epoch:   49/50    Loss: 3.468577281475067\n","\n","Epoch:   49/50    Loss: 3.458042013168335\n","\n","Epoch:   49/50    Loss: 3.4851600008010863\n","\n","Epoch:   49/50    Loss: 3.4661917963027955\n","\n","Epoch:   49/50    Loss: 3.489520230770111\n","\n","Epoch:   50/50    Loss: 3.455269944555941\n","\n","Epoch:   50/50    Loss: 3.415357623577118\n","\n","Epoch:   50/50    Loss: 3.4266279578208922\n","\n","Epoch:   50/50    Loss: 3.4345579771995545\n","\n","Epoch:   50/50    Loss: 3.4563387022018435\n","\n","Epoch:   50/50    Loss: 3.4527918939590454\n","\n","Epoch:   50/50    Loss: 3.460980473518372\n","\n","Epoch:   50/50    Loss: 3.4613446235656737\n","\n","Epoch:   50/50    Loss: 3.4576222648620605\n","\n","Epoch:   50/50    Loss: 3.4734733853340147\n","\n","Epoch:   50/50    Loss: 3.4619818058013916\n","\n","Epoch:   50/50    Loss: 3.47430145406723\n","\n","Epoch:   50/50    Loss: 3.4938270959854125\n","\n","Model Trained and Saved\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BEzp_gvsgO7c","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}