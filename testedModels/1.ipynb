{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"1.ipynb","provenance":[{"file_id":"18QvIHicyw5C-0ilk7hjZ_-ntievwHAsS","timestamp":1590807306832}],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"9kyuHNH2gO4b","colab_type":"text"},"source":["# 1. Import Modules"]},{"cell_type":"code","metadata":{"id":"NCgQckobgO4f","colab_type":"code","colab":{}},"source":["import numpy as np\n","import torch\n","import os\n","import pickle\n","\n","import torch.nn.functional as F\n","\n","from torch import nn\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","import helper\n","# import problem_unittests as tests"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LMa3EQndgmKu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":124},"outputId":"81cd0b4c-d3de-44f0-a44c-22406f709641","executionInfo":{"status":"ok","timestamp":1590852595530,"user_tz":420,"elapsed":29089,"user":{"displayName":"Frances Yu","photoUrl":"","userId":"14628227363021680713"}}},"source":["from google.colab import drive\n","\n","drive.mount('/content/gdrive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lU0EIV4Hij3K","colab_type":"code","colab":{}},"source":["SPECIAL_WORDS = {'PADDING': '<PAD>'}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6LSlg_Z-gO4v","colab_type":"text"},"source":["# 2. Explore the Data"]},{"cell_type":"code","metadata":{"id":"9M83fuYuhyhj","colab_type":"code","colab":{}},"source":["def load_data(path):\n","    \"\"\"\n","    Load Dataset from File\n","    \"\"\"\n","    input_file = os.path.join(path)\n","    with open(input_file, \"r\") as f:\n","        data = f.read()\n","\n","    return data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cGM_GDt5iO1g","colab_type":"code","colab":{}},"source":["def preprocess_and_save_data(dataset_path, token_lookup, create_lookup_tables):\n","    \"\"\"\n","    Preprocess Text Data\n","    \"\"\"\n","    text = load_data(dataset_path)\n","    \n","    # Ignore notice, since we don't use it for analysing the data\n","    text = text[81:]\n","\n","    token_dict = token_lookup()\n","    for key, token in token_dict.items():\n","        text = text.replace(key, ' {} '.format(token))\n","\n","    text = text.lower()\n","    text = text.split()\n","\n","    vocab_to_int, int_to_vocab = create_lookup_tables(text + list(SPECIAL_WORDS.values()))\n","    int_text = [vocab_to_int[word] for word in text]\n","    pickle.dump((int_text, vocab_to_int, int_to_vocab, token_dict), open('gdrive/My Drive/Lab/preprocess.p', 'wb'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xwo1eoaEiwHx","colab_type":"code","colab":{}},"source":["def load_preprocess():\n","    \"\"\"\n","    Load the Preprocessed Training data and return them in batches of <batch_size> or less\n","    \"\"\"\n","    return pickle.load(open('gdrive/My Drive/Lab/preprocess.p', mode='rb'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SZF87z9ZjO44","colab_type":"code","colab":{}},"source":["def save_model(filename, decoder):\n","    save_filename = os.path.splitext(os.path.basename(filename))[0] + '.pt'\n","    torch.save(decoder, save_filename)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U516D_QlgO42","colab_type":"code","colab":{}},"source":["data_dir = 'gdrive/My Drive/Lab/data/Seinfeld_Scripts.txt'\n","text = load_data(data_dir)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_XSjkzP9gO5C","colab_type":"code","outputId":"788df4c2-e831-4a2c-a75b-1b69a15f1eb4","colab":{"base_uri":"https://localhost:8080/","height":315},"executionInfo":{"status":"ok","timestamp":1590852617359,"user_tz":420,"elapsed":504,"user":{"displayName":"Frances Yu","photoUrl":"","userId":"14628227363021680713"}}},"source":["view_line_range = (0, 10)\n","\n","print('Dataset Stats')\n","print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n","\n","lines = text.split('\\n')\n","print('Number of lines: {}'.format(len(lines)))\n","word_count_line = [len(line.split()) for line in lines]\n","print('Average number of words in each line: {}'.format(np.average(word_count_line)))\n","\n","print()\n","print('The lines {} to {}:'.format(*view_line_range))\n","print('\\n'.join(text.split('\\n')[view_line_range[0]:view_line_range[1]]))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Dataset Stats\n","Roughly the number of unique words: 46367\n","Number of lines: 109233\n","Average number of words in each line: 5.544240293684143\n","\n","The lines 0 to 10:\n","jerry: do you know what this is all about? do you know, why were here? to be out, this is out...and out is one of the single most enjoyable experiences of life. people...did you ever hear people talking about we should go out? this is what theyre talking about...this whole thing, were all out now, no one is home. not one person here is home, were all out! there are people trying to find us, they dont know where we are. (on an imaginary phone) did you ring?, i cant find him. where did he go? he didnt tell me where he was going. he must have gone out. you wanna go out you get ready, you pick out the clothes, right? you take the shower, you get all ready, get the cash, get your friends, the car, the spot, the reservation...then youre standing around, what do you do? you go we gotta be getting back. once youre out, you wanna get back! you wanna go to sleep, you wanna get up, you wanna go out again tomorrow, right? where ever you are in life, its my feeling, youve gotta go. \n","\n","jerry: (pointing at georges shirt) see, to me, that button is in the worst possible spot. the second button literally makes or breaks the shirt, look at it. its too high! its in no-mans-land. you look like you live with your mother. \n","\n","george: are you through? \n","\n","jerry: you do of course try on, when you buy? \n","\n","george: yes, it was purple, i liked it, i dont actually recall considering the buttons. \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cMZ6xJoYgO5R","colab_type":"text"},"source":["# 3. Implement Pre-processing Functions"]},{"cell_type":"markdown","metadata":{"id":"S_HCdCzDgO5S","colab_type":"text"},"source":["### Lookup Table"]},{"cell_type":"code","metadata":{"id":"zH2DNdwGgO5V","colab_type":"code","colab":{}},"source":["def create_lookup_tables(text):\n","    \"\"\"\n","    Create lookup tables for vocabulary\n","    :param text: The text of tv scripts split into words\n","    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n","    \"\"\"\n","    \n","    vocab_to_int = dict()\n","    int_to_vocab = dict()\n","    \n","    sorted_word_set = sorted(set(text))\n","    \n","    for i, word in enumerate(sorted_word_set):\n","        vocab_to_int[word] = i\n","        int_to_vocab[i] = word\n","    \n","    return (vocab_to_int, int_to_vocab)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kQXrG9uAgO5f","colab_type":"code","outputId":"f17d9b35-3c5b-4418-c63a-6668c4b33fc8","colab":{}},"source":["tests.test_create_lookup_tables(create_lookup_tables)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Tests Passed\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IxzvAd5rgO5r","colab_type":"text"},"source":["### Tokenize Punctuation"]},{"cell_type":"code","metadata":{"id":"kYD7Qf70gO5r","colab_type":"code","colab":{}},"source":["def token_lookup():\n","    \"\"\"\n","    Generate a dict to turn punctuation into a token.\n","    :return: Tokenized dictionary where the key is the punctuation and the value is the token\n","    \"\"\"\n","    token_dict = dict()\n","    \n","    punctuation_list = [\n","        '.', \n","        ',', \n","        '\"', \n","        ';', \n","        '!', \n","        '?', \n","        '(', \n","        ')',\n","        '-',\n","        '\\n'\n","    ]\n","    \n","    token_list = [\n","        '||Period||', \n","        '||Comma||', \n","        '||Quotation_Mark||', \n","        '||Semicolon||', \n","        '||Exclamation_Mark||', \n","        '||Question_Mark||', \n","        '||Left_Parentheses||', \n","        '||Right_Parentheses||', \n","        '||Dash||', \n","        '||Return||'\n","    ]\n","    \n","    for (punctuation, token) in zip(punctuation_list, token_list):\n","        token_dict[punctuation] = token\n","        \n","    return token_dict"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gMSDBfGXgO50","colab_type":"code","outputId":"3bc34a9f-4255-4e94-a374-c989867b755c","colab":{}},"source":["tests.test_tokenize(token_lookup)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Tests Passed\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"niBXV40EgO58","colab_type":"text"},"source":["## Pre-process & Save Data"]},{"cell_type":"code","metadata":{"id":"wT6LRcoIgO5-","colab_type":"code","colab":{}},"source":["preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2L3nJN2bgO6G","colab_type":"text"},"source":["# **************** #1 Check Point ****************"]},{"cell_type":"code","metadata":{"id":"b5HuaioygO6H","colab_type":"code","colab":{}},"source":["int_text, vocab_to_int, int_to_vocab, token_dict = load_preprocess()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"URxy4KAmgO6N","colab_type":"text"},"source":["### Check Access to GPU"]},{"cell_type":"code","metadata":{"id":"OvnuuJZVgO6P","colab_type":"code","colab":{}},"source":["train_on_gpu = torch.cuda.is_available()\n","if not train_on_gpu:\n","    print('No GPU found. Please use a GPU to train your neural network.')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7cG8j4K-gO6Y","colab_type":"text"},"source":["### Batch Input Data"]},{"cell_type":"code","metadata":{"id":"cvScOkgJgO6Z","colab_type":"code","colab":{}},"source":["def batch_data(words, sequence_length, batch_size):\n","    \"\"\"\n","    Batch the neural network data using DataLoader\n","    :param words: The word ids of the TV scripts\n","    :param sequence_length: The sequence length of each batch\n","    :param batch_size: The size of each batch; the number of sequences in a batch\n","    :return: DataLoader with batched data\n","    \"\"\"\n","    \n","    ###\n","    \n","    words_len = len(words)\n","    \n","    batch_size_total = batch_size * sequence_length\n","    \n","    n_batches = words_len // batch_size_total\n","    \n","    words = words[:n_batches * batch_size_total]\n","    \n","    x,y = [],[]\n","    \n","    for idx in range(0,len(words) - sequence_length):\n","        x.append(words[idx:idx + sequence_length])\n","        y.append(words[idx+sequence_length])\n","        \n","    feature_arr = np.asarray(x)\n","    target_arr = np.asarray(y)\n","        \n","    ###\n","    \n","    feature_tensors = torch.from_numpy(feature_arr)\n","    target_tensors = torch.from_numpy(target_arr)\n","    \n","    ###\n","    \n","    data = TensorDataset(feature_tensors, target_tensors)\n","    data_loader = torch.utils.data.DataLoader(\n","        data, \n","        shuffle=True,\n","        batch_size=batch_size\n","    )\n","        \n","        \n","    return data_loader"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FO6fBrxZgO6j","colab_type":"code","outputId":"0debf90b-96dc-425b-d8e4-89ef7d50ba23","colab":{"base_uri":"https://localhost:8080/","height":121},"executionInfo":{"status":"ok","timestamp":1590852648893,"user_tz":420,"elapsed":408,"user":{"displayName":"Frances Yu","photoUrl":"","userId":"14628227363021680713"}}},"source":["test_text = range(50)\n","t_loader = batch_data(test_text, sequence_length=5, batch_size=2)\n","\n","data_iter = iter(t_loader)\n","sample_x, sample_y = data_iter.next()\n","\n","print(sample_x.shape)\n","print(sample_x)\n","print()\n","print(sample_y.shape)\n","print(sample_y)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["torch.Size([2, 5])\n","tensor([[ 9, 10, 11, 12, 13],\n","        [44, 45, 46, 47, 48]])\n","\n","torch.Size([2])\n","tensor([14, 49])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NcVKegk7gO6q","colab_type":"text"},"source":["# 4. Build the Neural Network"]},{"cell_type":"code","metadata":{"id":"GorXQeoBgO6r","colab_type":"code","colab":{}},"source":["class RNN(nn.Module):\n","    \n","    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):\n","        \"\"\"\n","        Initialize the PyTorch RNN Module\n","        :param vocab_size: The number of input dimensions of the neural network (the size of the vocabulary)\n","        :param output_size: The number of output dimensions of the neural network\n","        :param embedding_dim: The size of embeddings, should you choose to use them        \n","        :param hidden_dim: The size of the hidden layer outputs\n","        :param dropout: dropout to add in between LSTM/GRU layers\n","        \"\"\"\n","        super(RNN, self).__init__()\n","        \n","        # set class variables\n","        self.input_dim = vocab_size\n","        self.output_dim = output_size\n","        self.n_hidden = hidden_dim\n","        self.embedding_dim = embedding_dim\n","        self.n_layers = n_layers\n","        \n","        # define model layers\n","        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm = nn.LSTM(\n","            embedding_dim, \n","            hidden_dim, \n","            batch_first=True, \n","            num_layers=n_layers,\n","            dropout=dropout)\n","        self.linear = nn.Linear(hidden_dim, output_size)\n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def init_hidden(self, batch_size):\n","        ''' Initializes hidden state '''\n","        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n","        # initialized to zero, for hidden state and cell state of LSTM\n","        weight = next(self.parameters()).data\n","        \n","        if (train_on_gpu):\n","            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n","                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n","        else:\n","            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n","                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n","        \n","        return hidden\n","    \n","    def forward(self, nn_input, hidden):\n","        \"\"\"\n","        Forward propagation of the neural network\n","        :param nn_input: The input to the neural network\n","        :param hidden: The hidden state        \n","        :return: Two Tensors, the output of the neural network and the latest hidden state\n","        \"\"\"\n","        \n","        batch_size = nn_input.size(0)\n","        \n","        embeds = self.embeddings(nn_input)\n","        \n","        lstm_out, hidden = self.lstm(embeds, hidden)\n","        \n","        lstm_out = self.dropout(lstm_out)\n","        \n","        lstm_out = lstm_out.contiguous().view(-1, self.n_hidden)\n","        \n","        output = self.linear(lstm_out)\n","        \n","        output = output.view(batch_size, -1, self.output_dim)\n","        \n","        out = output[:, -1]\n","        \n","        return out, hidden"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PY5xiWrJgO6y","colab_type":"code","outputId":"44fea252-2fbf-43df-b3e7-855ca97b43f5","colab":{}},"source":["tests.test_rnn(RNN, train_on_gpu)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Tests Passed\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"38jN5u20gO64","colab_type":"text"},"source":["### Define forward and backpropagation"]},{"cell_type":"code","metadata":{"id":"OaIsdM4lgO66","colab_type":"code","colab":{}},"source":["def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):\n","    \"\"\"\n","    Forward and backward propagation on the neural network\n","    :param rnn: The PyTorch Module that holds the neural network\n","    :param optimizer: The PyTorch optimizer for the neural network\n","    :param criterion: The PyTorch loss function\n","    :param inp: A batch of input to the neural network\n","    :param target: The target output for the batch of input\n","    :return: The loss and the latest hidden state Tensor\n","    \"\"\"\n","    \n","    if train_on_gpu:\n","        inp, target = inp.cuda(), target.cuda()\n","\n","    # Creating new variables for the hidden state, otherwise\n","    # we'd backprop through the entire training history\n","    hidden = tuple([each.data for each in hidden])\n","    \n","    # zero accumulated gradients\n","    rnn.zero_grad()\n","    \n","    # get the output from the model\n","    output, hidden = rnn(inp, hidden)\n","    \n","    # calculate the loss and perform backprop\n","    loss = criterion(output, target)\n","    loss.backward()\n","     \n","    optimizer.step()\n","    \n","    return loss.item(), hidden"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PzBxFizHgO6_","colab_type":"code","outputId":"d60ae0eb-48f3-4f6d-e592-d31e10d2e706","colab":{}},"source":["tests.test_forward_back_prop(RNN, forward_back_prop, train_on_gpu)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Tests Passed\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"R4eWESkZgO7F","colab_type":"text"},"source":["# 5. Neural Network Training"]},{"cell_type":"markdown","metadata":{"id":"VE2iCyuPgO7H","colab_type":"text"},"source":["### Train Loop"]},{"cell_type":"code","metadata":{"id":"nH4wn49ugO7J","colab_type":"code","colab":{}},"source":["def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):\n","    batch_losses = []\n","    \n","    rnn.train()\n","\n","    print(\"Training for %d epoch(s)...\" % n_epochs)\n","    for epoch_i in range(1, n_epochs + 1):\n","        \n","        # initialize hidden state\n","        hidden = rnn.init_hidden(batch_size)\n","        \n","        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n","            \n","            # make sure you iterate over completely full batches, only\n","            n_batches = len(train_loader.dataset)//batch_size\n","            if(batch_i > n_batches):\n","                break\n","            \n","            # forward, back prop\n","            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)          \n","            # record loss\n","            batch_losses.append(loss)\n","\n","            # printing loss stats\n","            if batch_i % show_every_n_batches == 0:\n","                print('Epoch: {:>4}/{:<4}  Loss: {}\\n'.format(\n","                    epoch_i, n_epochs, np.average(batch_losses)))\n","                batch_losses = []\n","\n","    # returns a trained rnn\n","    return rnn"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0_HwJ90xgO7N","colab_type":"code","colab":{}},"source":["# Data params\n","# Sequence Length\n","sequence_length = 16  # of words in a sequence\n","# Batch Size\n","batch_size = 128\n","\n","# data loader - do not change\n","train_loader = batch_data(int_text, sequence_length, batch_size)\n","\n","# Training parameters\n","# Number of Epochs\n","num_epochs = 50\n","# Learning Rate\n","learning_rate = 0.0005\n","\n","# Model parameters\n","# Vocab size\n","vocab_size = len(vocab_to_int)\n","# Output size\n","output_size = vocab_size\n","# Embedding Dimension\n","embedding_dim = 250\n","# Hidden Dimension\n","hidden_dim = 256\n","# Number of RNN Layers\n","n_layers = 2\n","\n","# Show stats for every n number of batches\n","show_every_n_batches = 500"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Icex5HKgO7X","colab_type":"text"},"source":["### Train"]},{"cell_type":"code","metadata":{"id":"ZffH_HvygO7Z","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"46bd3d19-6c6e-4397-db17-4fba67e1b829","executionInfo":{"status":"ok","timestamp":1590859826795,"user_tz":420,"elapsed":7124595,"user":{"displayName":"Frances Yu","photoUrl":"","userId":"14628227363021680713"}}},"source":["rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5)\n","if train_on_gpu:\n","    rnn.cuda()\n","\n","# defining loss and optimization functions for training\n","optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n","criterion = nn.CrossEntropyLoss()\n","\n","# training the model\n","trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches)\n","\n","# saving the trained model\n","save_model('gdrive/My Drive/Lab/save/trained_rnn', trained_rnn)\n","print('Model Trained and Saved')"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Training for 50 epoch(s)...\n","Epoch:    1/50    Loss: 5.711249412536621\n","\n","Epoch:    1/50    Loss: 5.082968925476075\n","\n","Epoch:    1/50    Loss: 4.919073328971863\n","\n","Epoch:    1/50    Loss: 4.797498863697052\n","\n","Epoch:    1/50    Loss: 4.697561295509338\n","\n","Epoch:    1/50    Loss: 4.624488952636718\n","\n","Epoch:    1/50    Loss: 4.586837390422821\n","\n","Epoch:    1/50    Loss: 4.527997499465942\n","\n","Epoch:    1/50    Loss: 4.470246723175049\n","\n","Epoch:    1/50    Loss: 4.4731695203781126\n","\n","Epoch:    1/50    Loss: 4.466984477043152\n","\n","Epoch:    1/50    Loss: 4.4267758660316465\n","\n","Epoch:    1/50    Loss: 4.4036310458183285\n","\n","Epoch:    2/50    Loss: 4.347306964544111\n","\n","Epoch:    2/50    Loss: 4.294382278442383\n","\n","Epoch:    2/50    Loss: 4.285514365196228\n","\n","Epoch:    2/50    Loss: 4.256610577106476\n","\n","Epoch:    2/50    Loss: 4.245897693157196\n","\n","Epoch:    2/50    Loss: 4.264202850341797\n","\n","Epoch:    2/50    Loss: 4.247508197307587\n","\n","Epoch:    2/50    Loss: 4.23816818523407\n","\n","Epoch:    2/50    Loss: 4.244073395252228\n","\n","Epoch:    2/50    Loss: 4.235310331821442\n","\n","Epoch:    2/50    Loss: 4.227635905265808\n","\n","Epoch:    2/50    Loss: 4.198904430389404\n","\n","Epoch:    2/50    Loss: 4.212989731311798\n","\n","Epoch:    3/50    Loss: 4.176332882472447\n","\n","Epoch:    3/50    Loss: 4.108008959770203\n","\n","Epoch:    3/50    Loss: 4.116869109153748\n","\n","Epoch:    3/50    Loss: 4.117034271240234\n","\n","Epoch:    3/50    Loss: 4.119556143283844\n","\n","Epoch:    3/50    Loss: 4.101350515365601\n","\n","Epoch:    3/50    Loss: 4.122589740276337\n","\n","Epoch:    3/50    Loss: 4.137883430957794\n","\n","Epoch:    3/50    Loss: 4.140115023612976\n","\n","Epoch:    3/50    Loss: 4.100743183612823\n","\n","Epoch:    3/50    Loss: 4.11591649723053\n","\n","Epoch:    3/50    Loss: 4.101049170017243\n","\n","Epoch:    3/50    Loss: 4.102945651531219\n","\n","Epoch:    4/50    Loss: 4.0969975034437285\n","\n","Epoch:    4/50    Loss: 4.032189186573029\n","\n","Epoch:    4/50    Loss: 4.059501924991608\n","\n","Epoch:    4/50    Loss: 4.032348951816559\n","\n","Epoch:    4/50    Loss: 4.004882398605346\n","\n","Epoch:    4/50    Loss: 4.081019862651825\n","\n","Epoch:    4/50    Loss: 4.040254627704621\n","\n","Epoch:    4/50    Loss: 4.035960915088654\n","\n","Epoch:    4/50    Loss: 4.066086135864258\n","\n","Epoch:    4/50    Loss: 4.0355235581398015\n","\n","Epoch:    4/50    Loss: 4.030185875892639\n","\n","Epoch:    4/50    Loss: 4.055873787403106\n","\n","Epoch:    4/50    Loss: 4.057431704998017\n","\n","Epoch:    5/50    Loss: 3.9845515373973828\n","\n","Epoch:    5/50    Loss: 3.9700025386810305\n","\n","Epoch:    5/50    Loss: 3.9714278240203855\n","\n","Epoch:    5/50    Loss: 3.991668167591095\n","\n","Epoch:    5/50    Loss: 3.9789417972564696\n","\n","Epoch:    5/50    Loss: 3.9819864830970766\n","\n","Epoch:    5/50    Loss: 3.9892454051971438\n","\n","Epoch:    5/50    Loss: 4.0000815386772155\n","\n","Epoch:    5/50    Loss: 3.976285403728485\n","\n","Epoch:    5/50    Loss: 4.004661540508271\n","\n","Epoch:    5/50    Loss: 3.9859299192428588\n","\n","Epoch:    5/50    Loss: 4.005397490501403\n","\n","Epoch:    5/50    Loss: 4.0146306009292605\n","\n","Epoch:    6/50    Loss: 3.9529678846424887\n","\n","Epoch:    6/50    Loss: 3.9509409103393556\n","\n","Epoch:    6/50    Loss: 3.9372087473869324\n","\n","Epoch:    6/50    Loss: 3.926941205024719\n","\n","Epoch:    6/50    Loss: 3.954146201133728\n","\n","Epoch:    6/50    Loss: 3.9419889092445373\n","\n","Epoch:    6/50    Loss: 3.9605674238204958\n","\n","Epoch:    6/50    Loss: 3.929858836174011\n","\n","Epoch:    6/50    Loss: 3.944451325893402\n","\n","Epoch:    6/50    Loss: 3.965086492061615\n","\n","Epoch:    6/50    Loss: 3.935158092021942\n","\n","Epoch:    6/50    Loss: 3.955169982910156\n","\n","Epoch:    6/50    Loss: 3.957994245529175\n","\n","Epoch:    7/50    Loss: 3.9339597468828633\n","\n","Epoch:    7/50    Loss: 3.8781779084205628\n","\n","Epoch:    7/50    Loss: 3.9021289219856263\n","\n","Epoch:    7/50    Loss: 3.9039335684776306\n","\n","Epoch:    7/50    Loss: 3.9201091442108154\n","\n","Epoch:    7/50    Loss: 3.9053446764945985\n","\n","Epoch:    7/50    Loss: 3.922040403366089\n","\n","Epoch:    7/50    Loss: 3.915146306991577\n","\n","Epoch:    7/50    Loss: 3.917398765087128\n","\n","Epoch:    7/50    Loss: 3.9004512672424316\n","\n","Epoch:    7/50    Loss: 3.9275385103225706\n","\n","Epoch:    7/50    Loss: 3.940860534667969\n","\n","Epoch:    7/50    Loss: 3.9071980214118955\n","\n","Epoch:    8/50    Loss: 3.878241001254451\n","\n","Epoch:    8/50    Loss: 3.8707686796188354\n","\n","Epoch:    8/50    Loss: 3.8748662190437315\n","\n","Epoch:    8/50    Loss: 3.887922351360321\n","\n","Epoch:    8/50    Loss: 3.8712357873916625\n","\n","Epoch:    8/50    Loss: 3.8856187324523925\n","\n","Epoch:    8/50    Loss: 3.8767504434585573\n","\n","Epoch:    8/50    Loss: 3.876169408798218\n","\n","Epoch:    8/50    Loss: 3.885334237575531\n","\n","Epoch:    8/50    Loss: 3.877929271221161\n","\n","Epoch:    8/50    Loss: 3.898395782470703\n","\n","Epoch:    8/50    Loss: 3.901412858963013\n","\n","Epoch:    8/50    Loss: 3.899267888069153\n","\n","Epoch:    9/50    Loss: 3.853240653546187\n","\n","Epoch:    9/50    Loss: 3.8582137813568114\n","\n","Epoch:    9/50    Loss: 3.8448821244239806\n","\n","Epoch:    9/50    Loss: 3.825966670036316\n","\n","Epoch:    9/50    Loss: 3.8308008155822755\n","\n","Epoch:    9/50    Loss: 3.8560604310035704\n","\n","Epoch:    9/50    Loss: 3.85064861536026\n","\n","Epoch:    9/50    Loss: 3.8645152020454407\n","\n","Epoch:    9/50    Loss: 3.8641886529922487\n","\n","Epoch:    9/50    Loss: 3.8772794423103334\n","\n","Epoch:    9/50    Loss: 3.859254332065582\n","\n","Epoch:    9/50    Loss: 3.8749538154602052\n","\n","Epoch:    9/50    Loss: 3.8759610872268677\n","\n","Epoch:   10/50    Loss: 3.849664494193259\n","\n","Epoch:   10/50    Loss: 3.8217607908248903\n","\n","Epoch:   10/50    Loss: 3.816819754600525\n","\n","Epoch:   10/50    Loss: 3.8207186017036436\n","\n","Epoch:   10/50    Loss: 3.8310912356376647\n","\n","Epoch:   10/50    Loss: 3.8318507475852965\n","\n","Epoch:   10/50    Loss: 3.817455080509186\n","\n","Epoch:   10/50    Loss: 3.8434944224357603\n","\n","Epoch:   10/50    Loss: 3.80970077419281\n","\n","Epoch:   10/50    Loss: 3.818305525302887\n","\n","Epoch:   10/50    Loss: 3.860556948184967\n","\n","Epoch:   10/50    Loss: 3.846706646442413\n","\n","Epoch:   10/50    Loss: 3.8859760785102844\n","\n","Epoch:   11/50    Loss: 3.8206403944116936\n","\n","Epoch:   11/50    Loss: 3.777626995563507\n","\n","Epoch:   11/50    Loss: 3.790769307136536\n","\n","Epoch:   11/50    Loss: 3.8118563232421874\n","\n","Epoch:   11/50    Loss: 3.825718204975128\n","\n","Epoch:   11/50    Loss: 3.8156345133781433\n","\n","Epoch:   11/50    Loss: 3.7933478021621703\n","\n","Epoch:   11/50    Loss: 3.7976807279586793\n","\n","Epoch:   11/50    Loss: 3.8153005652427674\n","\n","Epoch:   11/50    Loss: 3.8012246985435487\n","\n","Epoch:   11/50    Loss: 3.8216644144058227\n","\n","Epoch:   11/50    Loss: 3.830670351982117\n","\n","Epoch:   11/50    Loss: 3.8290336165428163\n","\n","Epoch:   12/50    Loss: 3.812458238711074\n","\n","Epoch:   12/50    Loss: 3.7511722002029417\n","\n","Epoch:   12/50    Loss: 3.7791962361335756\n","\n","Epoch:   12/50    Loss: 3.7781479449272157\n","\n","Epoch:   12/50    Loss: 3.780967631340027\n","\n","Epoch:   12/50    Loss: 3.797359752655029\n","\n","Epoch:   12/50    Loss: 3.7808728346824645\n","\n","Epoch:   12/50    Loss: 3.7902030611038207\n","\n","Epoch:   12/50    Loss: 3.8068535580635072\n","\n","Epoch:   12/50    Loss: 3.8099391384124757\n","\n","Epoch:   12/50    Loss: 3.7999848289489746\n","\n","Epoch:   12/50    Loss: 3.8140442309379576\n","\n","Epoch:   12/50    Loss: 3.8270133113861085\n","\n","Epoch:   13/50    Loss: 3.7783800852062557\n","\n","Epoch:   13/50    Loss: 3.736863144874573\n","\n","Epoch:   13/50    Loss: 3.767558138847351\n","\n","Epoch:   13/50    Loss: 3.75929953622818\n","\n","Epoch:   13/50    Loss: 3.752418363571167\n","\n","Epoch:   13/50    Loss: 3.769318836688995\n","\n","Epoch:   13/50    Loss: 3.7683147892951965\n","\n","Epoch:   13/50    Loss: 3.7801726751327513\n","\n","Epoch:   13/50    Loss: 3.7912963562011717\n","\n","Epoch:   13/50    Loss: 3.766258162021637\n","\n","Epoch:   13/50    Loss: 3.78071998500824\n","\n","Epoch:   13/50    Loss: 3.8044648938179018\n","\n","Epoch:   13/50    Loss: 3.798791021347046\n","\n","Epoch:   14/50    Loss: 3.774109697192752\n","\n","Epoch:   14/50    Loss: 3.725971121788025\n","\n","Epoch:   14/50    Loss: 3.7536748442649843\n","\n","Epoch:   14/50    Loss: 3.7461456809043883\n","\n","Epoch:   14/50    Loss: 3.760725266933441\n","\n","Epoch:   14/50    Loss: 3.744785943031311\n","\n","Epoch:   14/50    Loss: 3.7447700872421263\n","\n","Epoch:   14/50    Loss: 3.761843342781067\n","\n","Epoch:   14/50    Loss: 3.7616213898658755\n","\n","Epoch:   14/50    Loss: 3.7784736914634705\n","\n","Epoch:   14/50    Loss: 3.7731029915809633\n","\n","Epoch:   14/50    Loss: 3.773515703678131\n","\n","Epoch:   14/50    Loss: 3.782073043346405\n","\n","Epoch:   15/50    Loss: 3.738650398284228\n","\n","Epoch:   15/50    Loss: 3.6969445667266845\n","\n","Epoch:   15/50    Loss: 3.728644744873047\n","\n","Epoch:   15/50    Loss: 3.7293206896781923\n","\n","Epoch:   15/50    Loss: 3.7302240447998045\n","\n","Epoch:   15/50    Loss: 3.7595803999900816\n","\n","Epoch:   15/50    Loss: 3.7509823412895202\n","\n","Epoch:   15/50    Loss: 3.742675889968872\n","\n","Epoch:   15/50    Loss: 3.755507157802582\n","\n","Epoch:   15/50    Loss: 3.742538567066193\n","\n","Epoch:   15/50    Loss: 3.7666763100624086\n","\n","Epoch:   15/50    Loss: 3.7460009198188784\n","\n","Epoch:   15/50    Loss: 3.7597967977523803\n","\n","Epoch:   16/50    Loss: 3.73258372218318\n","\n","Epoch:   16/50    Loss: 3.6997028040885924\n","\n","Epoch:   16/50    Loss: 3.6929967584609984\n","\n","Epoch:   16/50    Loss: 3.70002129983902\n","\n","Epoch:   16/50    Loss: 3.722621166706085\n","\n","Epoch:   16/50    Loss: 3.7170487565994264\n","\n","Epoch:   16/50    Loss: 3.7112563071250917\n","\n","Epoch:   16/50    Loss: 3.736127304553986\n","\n","Epoch:   16/50    Loss: 3.7311057562828065\n","\n","Epoch:   16/50    Loss: 3.7589789571762084\n","\n","Epoch:   16/50    Loss: 3.7330465574264524\n","\n","Epoch:   16/50    Loss: 3.768119444847107\n","\n","Epoch:   16/50    Loss: 3.7521241154670717\n","\n","Epoch:   17/50    Loss: 3.7163307209333114\n","\n","Epoch:   17/50    Loss: 3.674346909046173\n","\n","Epoch:   17/50    Loss: 3.701676357269287\n","\n","Epoch:   17/50    Loss: 3.7028766613006594\n","\n","Epoch:   17/50    Loss: 3.6877153100967406\n","\n","Epoch:   17/50    Loss: 3.707840883731842\n","\n","Epoch:   17/50    Loss: 3.6989071612358093\n","\n","Epoch:   17/50    Loss: 3.7141503443717956\n","\n","Epoch:   17/50    Loss: 3.7044361009597777\n","\n","Epoch:   17/50    Loss: 3.732263236045837\n","\n","Epoch:   17/50    Loss: 3.747660843372345\n","\n","Epoch:   17/50    Loss: 3.725285206317902\n","\n","Epoch:   17/50    Loss: 3.730831200122833\n","\n","Epoch:   18/50    Loss: 3.702997700389906\n","\n","Epoch:   18/50    Loss: 3.693723561286926\n","\n","Epoch:   18/50    Loss: 3.6794772319793703\n","\n","Epoch:   18/50    Loss: 3.6734745893478395\n","\n","Epoch:   18/50    Loss: 3.6686537976264955\n","\n","Epoch:   18/50    Loss: 3.687932054042816\n","\n","Epoch:   18/50    Loss: 3.6965277047157286\n","\n","Epoch:   18/50    Loss: 3.725171615600586\n","\n","Epoch:   18/50    Loss: 3.7065235376358032\n","\n","Epoch:   18/50    Loss: 3.7028916277885435\n","\n","Epoch:   18/50    Loss: 3.7245037479400636\n","\n","Epoch:   18/50    Loss: 3.709587071895599\n","\n","Epoch:   18/50    Loss: 3.6979151673316957\n","\n","Epoch:   19/50    Loss: 3.6800604028174724\n","\n","Epoch:   19/50    Loss: 3.6611413650512694\n","\n","Epoch:   19/50    Loss: 3.663184184551239\n","\n","Epoch:   19/50    Loss: 3.6819471955299377\n","\n","Epoch:   19/50    Loss: 3.6820894927978514\n","\n","Epoch:   19/50    Loss: 3.66525754070282\n","\n","Epoch:   19/50    Loss: 3.693420063495636\n","\n","Epoch:   19/50    Loss: 3.6934812831878663\n","\n","Epoch:   19/50    Loss: 3.688902506351471\n","\n","Epoch:   19/50    Loss: 3.7038935856819153\n","\n","Epoch:   19/50    Loss: 3.698869330883026\n","\n","Epoch:   19/50    Loss: 3.692628715515137\n","\n","Epoch:   19/50    Loss: 3.7154820103645325\n","\n","Epoch:   20/50    Loss: 3.6838102686268446\n","\n","Epoch:   20/50    Loss: 3.6262365140914916\n","\n","Epoch:   20/50    Loss: 3.623897610664368\n","\n","Epoch:   20/50    Loss: 3.6797605195045473\n","\n","Epoch:   20/50    Loss: 3.6585659952163696\n","\n","Epoch:   20/50    Loss: 3.676046856403351\n","\n","Epoch:   20/50    Loss: 3.66710259103775\n","\n","Epoch:   20/50    Loss: 3.6671543011665344\n","\n","Epoch:   20/50    Loss: 3.677733427524567\n","\n","Epoch:   20/50    Loss: 3.6860225052833555\n","\n","Epoch:   20/50    Loss: 3.667552959918976\n","\n","Epoch:   20/50    Loss: 3.704539662361145\n","\n","Epoch:   20/50    Loss: 3.7050298233032226\n","\n","Epoch:   21/50    Loss: 3.6687855772728466\n","\n","Epoch:   21/50    Loss: 3.626654079914093\n","\n","Epoch:   21/50    Loss: 3.6507603483200075\n","\n","Epoch:   21/50    Loss: 3.6288524446487425\n","\n","Epoch:   21/50    Loss: 3.6528665409088137\n","\n","Epoch:   21/50    Loss: 3.660229835033417\n","\n","Epoch:   21/50    Loss: 3.6434345049858092\n","\n","Epoch:   21/50    Loss: 3.669339422702789\n","\n","Epoch:   21/50    Loss: 3.6619028224945067\n","\n","Epoch:   21/50    Loss: 3.6480299763679502\n","\n","Epoch:   21/50    Loss: 3.673234046459198\n","\n","Epoch:   21/50    Loss: 3.694347013950348\n","\n","Epoch:   21/50    Loss: 3.679344472885132\n","\n","Epoch:   22/50    Loss: 3.6557959488459995\n","\n","Epoch:   22/50    Loss: 3.6143506264686582\n","\n","Epoch:   22/50    Loss: 3.657246166229248\n","\n","Epoch:   22/50    Loss: 3.6202803921699522\n","\n","Epoch:   22/50    Loss: 3.6506858530044557\n","\n","Epoch:   22/50    Loss: 3.642222824573517\n","\n","Epoch:   22/50    Loss: 3.641047306537628\n","\n","Epoch:   22/50    Loss: 3.648960590362549\n","\n","Epoch:   22/50    Loss: 3.649703172683716\n","\n","Epoch:   22/50    Loss: 3.67106019115448\n","\n","Epoch:   22/50    Loss: 3.666104681015015\n","\n","Epoch:   22/50    Loss: 3.670883288383484\n","\n","Epoch:   22/50    Loss: 3.6514361319541933\n","\n","Epoch:   23/50    Loss: 3.620746181207603\n","\n","Epoch:   23/50    Loss: 3.6238559789657594\n","\n","Epoch:   23/50    Loss: 3.604328571796417\n","\n","Epoch:   23/50    Loss: 3.6272349162101745\n","\n","Epoch:   23/50    Loss: 3.59238369846344\n","\n","Epoch:   23/50    Loss: 3.643927586078644\n","\n","Epoch:   23/50    Loss: 3.6499510827064516\n","\n","Epoch:   23/50    Loss: 3.648556025981903\n","\n","Epoch:   23/50    Loss: 3.6580906195640566\n","\n","Epoch:   23/50    Loss: 3.6459784865379334\n","\n","Epoch:   23/50    Loss: 3.6334106149673464\n","\n","Epoch:   23/50    Loss: 3.655130883216858\n","\n","Epoch:   23/50    Loss: 3.664180024623871\n","\n","Epoch:   24/50    Loss: 3.6287499423817624\n","\n","Epoch:   24/50    Loss: 3.588962601184845\n","\n","Epoch:   24/50    Loss: 3.60785803604126\n","\n","Epoch:   24/50    Loss: 3.6063117060661316\n","\n","Epoch:   24/50    Loss: 3.630011573314667\n","\n","Epoch:   24/50    Loss: 3.6111534271240235\n","\n","Epoch:   24/50    Loss: 3.628654562950134\n","\n","Epoch:   24/50    Loss: 3.5939863138198853\n","\n","Epoch:   24/50    Loss: 3.625150084018707\n","\n","Epoch:   24/50    Loss: 3.665900146484375\n","\n","Epoch:   24/50    Loss: 3.6488025431632995\n","\n","Epoch:   24/50    Loss: 3.6455147223472597\n","\n","Epoch:   24/50    Loss: 3.6444186868667603\n","\n","Epoch:   25/50    Loss: 3.6206896605904335\n","\n","Epoch:   25/50    Loss: 3.5790277614593506\n","\n","Epoch:   25/50    Loss: 3.576015929698944\n","\n","Epoch:   25/50    Loss: 3.6222668190002443\n","\n","Epoch:   25/50    Loss: 3.5930785098075866\n","\n","Epoch:   25/50    Loss: 3.621738079071045\n","\n","Epoch:   25/50    Loss: 3.608450813293457\n","\n","Epoch:   25/50    Loss: 3.596902190685272\n","\n","Epoch:   25/50    Loss: 3.616275743484497\n","\n","Epoch:   25/50    Loss: 3.6229714541435243\n","\n","Epoch:   25/50    Loss: 3.6509177923202514\n","\n","Epoch:   25/50    Loss: 3.6300206174850462\n","\n","Epoch:   25/50    Loss: 3.640789545536041\n","\n","Epoch:   26/50    Loss: 3.6112797934022014\n","\n","Epoch:   26/50    Loss: 3.5721972613334656\n","\n","Epoch:   26/50    Loss: 3.591943130970001\n","\n","Epoch:   26/50    Loss: 3.6055090675354005\n","\n","Epoch:   26/50    Loss: 3.569007888793945\n","\n","Epoch:   26/50    Loss: 3.592086633205414\n","\n","Epoch:   26/50    Loss: 3.6046589736938475\n","\n","Epoch:   26/50    Loss: 3.621428177833557\n","\n","Epoch:   26/50    Loss: 3.6096156854629515\n","\n","Epoch:   26/50    Loss: 3.644802487373352\n","\n","Epoch:   26/50    Loss: 3.614676254749298\n","\n","Epoch:   26/50    Loss: 3.6116827225685117\n","\n","Epoch:   26/50    Loss: 3.6060950236320495\n","\n","Epoch:   27/50    Loss: 3.595927233492122\n","\n","Epoch:   27/50    Loss: 3.5655053873062132\n","\n","Epoch:   27/50    Loss: 3.5815628943443296\n","\n","Epoch:   27/50    Loss: 3.575570583820343\n","\n","Epoch:   27/50    Loss: 3.59246138715744\n","\n","Epoch:   27/50    Loss: 3.583183822631836\n","\n","Epoch:   27/50    Loss: 3.5804248824119567\n","\n","Epoch:   27/50    Loss: 3.5908650317192077\n","\n","Epoch:   27/50    Loss: 3.604479838371277\n","\n","Epoch:   27/50    Loss: 3.6151550126075747\n","\n","Epoch:   27/50    Loss: 3.6282448749542238\n","\n","Epoch:   27/50    Loss: 3.609434230327606\n","\n","Epoch:   27/50    Loss: 3.6243065757751465\n","\n","Epoch:   28/50    Loss: 3.585383937803871\n","\n","Epoch:   28/50    Loss: 3.551559139728546\n","\n","Epoch:   28/50    Loss: 3.5826317257881164\n","\n","Epoch:   28/50    Loss: 3.587635102748871\n","\n","Epoch:   28/50    Loss: 3.5792073764801025\n","\n","Epoch:   28/50    Loss: 3.5588252301216126\n","\n","Epoch:   28/50    Loss: 3.588840525150299\n","\n","Epoch:   28/50    Loss: 3.5824850535392763\n","\n","Epoch:   28/50    Loss: 3.5813287048339846\n","\n","Epoch:   28/50    Loss: 3.6321333146095274\n","\n","Epoch:   28/50    Loss: 3.6114733567237853\n","\n","Epoch:   28/50    Loss: 3.602735251903534\n","\n","Epoch:   28/50    Loss: 3.598753065109253\n","\n","Epoch:   29/50    Loss: 3.5793365616743706\n","\n","Epoch:   29/50    Loss: 3.5430076909065247\n","\n","Epoch:   29/50    Loss: 3.5316550631523134\n","\n","Epoch:   29/50    Loss: 3.5521127223968505\n","\n","Epoch:   29/50    Loss: 3.5498317489624025\n","\n","Epoch:   29/50    Loss: 3.5843447518348692\n","\n","Epoch:   29/50    Loss: 3.573513689517975\n","\n","Epoch:   29/50    Loss: 3.561894724369049\n","\n","Epoch:   29/50    Loss: 3.566093966007233\n","\n","Epoch:   29/50    Loss: 3.6085725111961366\n","\n","Epoch:   29/50    Loss: 3.6006326227188112\n","\n","Epoch:   29/50    Loss: 3.6188664717674257\n","\n","Epoch:   29/50    Loss: 3.609836949825287\n","\n","Epoch:   30/50    Loss: 3.5708711368572725\n","\n","Epoch:   30/50    Loss: 3.5502926473617555\n","\n","Epoch:   30/50    Loss: 3.5309511709213255\n","\n","Epoch:   30/50    Loss: 3.5488440356254576\n","\n","Epoch:   30/50    Loss: 3.551005337715149\n","\n","Epoch:   30/50    Loss: 3.569598774433136\n","\n","Epoch:   30/50    Loss: 3.5646961350440978\n","\n","Epoch:   30/50    Loss: 3.5789716739654542\n","\n","Epoch:   30/50    Loss: 3.582267511844635\n","\n","Epoch:   30/50    Loss: 3.5997592296600343\n","\n","Epoch:   30/50    Loss: 3.556604821205139\n","\n","Epoch:   30/50    Loss: 3.584024875164032\n","\n","Epoch:   30/50    Loss: 3.5944799852371214\n","\n","Epoch:   31/50    Loss: 3.5638721735062266\n","\n","Epoch:   31/50    Loss: 3.5346415910720825\n","\n","Epoch:   31/50    Loss: 3.5394713110923766\n","\n","Epoch:   31/50    Loss: 3.5489700813293457\n","\n","Epoch:   31/50    Loss: 3.5475864100456236\n","\n","Epoch:   31/50    Loss: 3.548603946208954\n","\n","Epoch:   31/50    Loss: 3.572513635635376\n","\n","Epoch:   31/50    Loss: 3.5578334255218507\n","\n","Epoch:   31/50    Loss: 3.562280575752258\n","\n","Epoch:   31/50    Loss: 3.548410378456116\n","\n","Epoch:   31/50    Loss: 3.5840673003196715\n","\n","Epoch:   31/50    Loss: 3.5841257491111755\n","\n","Epoch:   31/50    Loss: 3.569599558353424\n","\n","Epoch:   32/50    Loss: 3.549425349867008\n","\n","Epoch:   32/50    Loss: 3.5285703024864197\n","\n","Epoch:   32/50    Loss: 3.506187497615814\n","\n","Epoch:   32/50    Loss: 3.5361795530319213\n","\n","Epoch:   32/50    Loss: 3.551024465560913\n","\n","Epoch:   32/50    Loss: 3.5319404778480528\n","\n","Epoch:   32/50    Loss: 3.5620686559677126\n","\n","Epoch:   32/50    Loss: 3.5558765268325807\n","\n","Epoch:   32/50    Loss: 3.558220694541931\n","\n","Epoch:   32/50    Loss: 3.587508146762848\n","\n","Epoch:   32/50    Loss: 3.5915067839622496\n","\n","Epoch:   32/50    Loss: 3.58430699300766\n","\n","Epoch:   32/50    Loss: 3.55576584482193\n","\n","Epoch:   33/50    Loss: 3.547424467810749\n","\n","Epoch:   33/50    Loss: 3.49359840965271\n","\n","Epoch:   33/50    Loss: 3.529900935649872\n","\n","Epoch:   33/50    Loss: 3.520197151184082\n","\n","Epoch:   33/50    Loss: 3.5387881479263306\n","\n","Epoch:   33/50    Loss: 3.5297830185890198\n","\n","Epoch:   33/50    Loss: 3.5456234822273256\n","\n","Epoch:   33/50    Loss: 3.5541218156814574\n","\n","Epoch:   33/50    Loss: 3.5711802420616148\n","\n","Epoch:   33/50    Loss: 3.5617488164901734\n","\n","Epoch:   33/50    Loss: 3.59005189704895\n","\n","Epoch:   33/50    Loss: 3.5372785868644714\n","\n","Epoch:   33/50    Loss: 3.5649741706848146\n","\n","Epoch:   34/50    Loss: 3.530427341043514\n","\n","Epoch:   34/50    Loss: 3.512510263442993\n","\n","Epoch:   34/50    Loss: 3.5127741189002992\n","\n","Epoch:   34/50    Loss: 3.5221073188781737\n","\n","Epoch:   34/50    Loss: 3.5082043666839597\n","\n","Epoch:   34/50    Loss: 3.527760305404663\n","\n","Epoch:   34/50    Loss: 3.5298831810951232\n","\n","Epoch:   34/50    Loss: 3.52699223947525\n","\n","Epoch:   34/50    Loss: 3.568775974750519\n","\n","Epoch:   34/50    Loss: 3.5423136777877806\n","\n","Epoch:   34/50    Loss: 3.5714218330383303\n","\n","Epoch:   34/50    Loss: 3.5671209988594055\n","\n","Epoch:   34/50    Loss: 3.5681130995750427\n","\n","Epoch:   35/50    Loss: 3.521660348297533\n","\n","Epoch:   35/50    Loss: 3.495399483680725\n","\n","Epoch:   35/50    Loss: 3.5049868173599243\n","\n","Epoch:   35/50    Loss: 3.4883299283981324\n","\n","Epoch:   35/50    Loss: 3.530045974254608\n","\n","Epoch:   35/50    Loss: 3.5198723316192626\n","\n","Epoch:   35/50    Loss: 3.5388458714485167\n","\n","Epoch:   35/50    Loss: 3.533973138809204\n","\n","Epoch:   35/50    Loss: 3.550499481678009\n","\n","Epoch:   35/50    Loss: 3.5251523776054383\n","\n","Epoch:   35/50    Loss: 3.5534163966178895\n","\n","Epoch:   35/50    Loss: 3.56186353969574\n","\n","Epoch:   35/50    Loss: 3.552287169456482\n","\n","Epoch:   36/50    Loss: 3.5239936947449655\n","\n","Epoch:   36/50    Loss: 3.49326486825943\n","\n","Epoch:   36/50    Loss: 3.5084173593521117\n","\n","Epoch:   36/50    Loss: 3.5222639999389647\n","\n","Epoch:   36/50    Loss: 3.5141064968109132\n","\n","Epoch:   36/50    Loss: 3.5075671544075013\n","\n","Epoch:   36/50    Loss: 3.512496216773987\n","\n","Epoch:   36/50    Loss: 3.531989347934723\n","\n","Epoch:   36/50    Loss: 3.535244132041931\n","\n","Epoch:   36/50    Loss: 3.535624147891998\n","\n","Epoch:   36/50    Loss: 3.5455729146003723\n","\n","Epoch:   36/50    Loss: 3.5209470572471617\n","\n","Epoch:   36/50    Loss: 3.573273200035095\n","\n","Epoch:   37/50    Loss: 3.4951656918829004\n","\n","Epoch:   37/50    Loss: 3.4692935557365416\n","\n","Epoch:   37/50    Loss: 3.4924957766532896\n","\n","Epoch:   37/50    Loss: 3.493456037521362\n","\n","Epoch:   37/50    Loss: 3.520547549724579\n","\n","Epoch:   37/50    Loss: 3.5107773303985597\n","\n","Epoch:   37/50    Loss: 3.526516518115997\n","\n","Epoch:   37/50    Loss: 3.524224492073059\n","\n","Epoch:   37/50    Loss: 3.547462899208069\n","\n","Epoch:   37/50    Loss: 3.535781319141388\n","\n","Epoch:   37/50    Loss: 3.514717398643494\n","\n","Epoch:   37/50    Loss: 3.5417473554611205\n","\n","Epoch:   37/50    Loss: 3.5429589428901673\n","\n","Epoch:   38/50    Loss: 3.498208942950331\n","\n","Epoch:   38/50    Loss: 3.4486493906974793\n","\n","Epoch:   38/50    Loss: 3.5136120734214784\n","\n","Epoch:   38/50    Loss: 3.4976935420036317\n","\n","Epoch:   38/50    Loss: 3.5006646337509157\n","\n","Epoch:   38/50    Loss: 3.527416214942932\n","\n","Epoch:   38/50    Loss: 3.5103773398399354\n","\n","Epoch:   38/50    Loss: 3.5038248229026796\n","\n","Epoch:   38/50    Loss: 3.4850284423828124\n","\n","Epoch:   38/50    Loss: 3.536289945602417\n","\n","Epoch:   38/50    Loss: 3.538236978530884\n","\n","Epoch:   38/50    Loss: 3.526704713344574\n","\n","Epoch:   38/50    Loss: 3.537880154132843\n","\n","Epoch:   39/50    Loss: 3.5119929437965496\n","\n","Epoch:   39/50    Loss: 3.4605670194625855\n","\n","Epoch:   39/50    Loss: 3.497093098163605\n","\n","Epoch:   39/50    Loss: 3.4865850400924683\n","\n","Epoch:   39/50    Loss: 3.4941776895523073\n","\n","Epoch:   39/50    Loss: 3.5116022176742554\n","\n","Epoch:   39/50    Loss: 3.5080055680274964\n","\n","Epoch:   39/50    Loss: 3.5065925421714783\n","\n","Epoch:   39/50    Loss: 3.5216817502975464\n","\n","Epoch:   39/50    Loss: 3.5262094559669497\n","\n","Epoch:   39/50    Loss: 3.498361135482788\n","\n","Epoch:   39/50    Loss: 3.5173418641090395\n","\n","Epoch:   39/50    Loss: 3.532390303611755\n","\n","Epoch:   40/50    Loss: 3.4948162498016675\n","\n","Epoch:   40/50    Loss: 3.44420969581604\n","\n","Epoch:   40/50    Loss: 3.4878713970184325\n","\n","Epoch:   40/50    Loss: 3.484115484714508\n","\n","Epoch:   40/50    Loss: 3.4943609714508055\n","\n","Epoch:   40/50    Loss: 3.4795238003730775\n","\n","Epoch:   40/50    Loss: 3.5031340255737304\n","\n","Epoch:   40/50    Loss: 3.499100250244141\n","\n","Epoch:   40/50    Loss: 3.5185064878463743\n","\n","Epoch:   40/50    Loss: 3.5152688331604005\n","\n","Epoch:   40/50    Loss: 3.5119734897613526\n","\n","Epoch:   40/50    Loss: 3.5110793342590334\n","\n","Epoch:   40/50    Loss: 3.5194489293098448\n","\n","Epoch:   41/50    Loss: 3.5011583155213355\n","\n","Epoch:   41/50    Loss: 3.468264784812927\n","\n","Epoch:   41/50    Loss: 3.4635793919563294\n","\n","Epoch:   41/50    Loss: 3.4627155618667604\n","\n","Epoch:   41/50    Loss: 3.4849635219573973\n","\n","Epoch:   41/50    Loss: 3.462475543498993\n","\n","Epoch:   41/50    Loss: 3.4857827844619753\n","\n","Epoch:   41/50    Loss: 3.5109782161712646\n","\n","Epoch:   41/50    Loss: 3.4842603583335876\n","\n","Epoch:   41/50    Loss: 3.5207159757614135\n","\n","Epoch:   41/50    Loss: 3.53002845287323\n","\n","Epoch:   41/50    Loss: 3.5112036275863647\n","\n","Epoch:   41/50    Loss: 3.523678774356842\n","\n","Epoch:   42/50    Loss: 3.4775883263915124\n","\n","Epoch:   42/50    Loss: 3.451772352695465\n","\n","Epoch:   42/50    Loss: 3.4643565402030947\n","\n","Epoch:   42/50    Loss: 3.462292371273041\n","\n","Epoch:   42/50    Loss: 3.48108474111557\n","\n","Epoch:   42/50    Loss: 3.478624773025513\n","\n","Epoch:   42/50    Loss: 3.4864158415794373\n","\n","Epoch:   42/50    Loss: 3.484325032711029\n","\n","Epoch:   42/50    Loss: 3.4933259620666504\n","\n","Epoch:   42/50    Loss: 3.516286838054657\n","\n","Epoch:   42/50    Loss: 3.5093926005363465\n","\n","Epoch:   42/50    Loss: 3.5051593322753907\n","\n","Epoch:   42/50    Loss: 3.5206287021636964\n","\n","Epoch:   43/50    Loss: 3.47424933037743\n","\n","Epoch:   43/50    Loss: 3.474441029548645\n","\n","Epoch:   43/50    Loss: 3.458158091545105\n","\n","Epoch:   43/50    Loss: 3.466119780063629\n","\n","Epoch:   43/50    Loss: 3.4662456493377687\n","\n","Epoch:   43/50    Loss: 3.4629820842742918\n","\n","Epoch:   43/50    Loss: 3.4611407356262207\n","\n","Epoch:   43/50    Loss: 3.4904723482131956\n","\n","Epoch:   43/50    Loss: 3.490913067340851\n","\n","Epoch:   43/50    Loss: 3.494448851108551\n","\n","Epoch:   43/50    Loss: 3.5011737837791443\n","\n","Epoch:   43/50    Loss: 3.4753745126724245\n","\n","Epoch:   43/50    Loss: 3.510679901123047\n","\n","Epoch:   44/50    Loss: 3.486950022833688\n","\n","Epoch:   44/50    Loss: 3.4407712197303773\n","\n","Epoch:   44/50    Loss: 3.4642238540649415\n","\n","Epoch:   44/50    Loss: 3.445703233718872\n","\n","Epoch:   44/50    Loss: 3.451520472049713\n","\n","Epoch:   44/50    Loss: 3.468470142841339\n","\n","Epoch:   44/50    Loss: 3.472202081680298\n","\n","Epoch:   44/50    Loss: 3.4683302121162414\n","\n","Epoch:   44/50    Loss: 3.4924556007385252\n","\n","Epoch:   44/50    Loss: 3.5021298837661745\n","\n","Epoch:   44/50    Loss: 3.4811384525299074\n","\n","Epoch:   44/50    Loss: 3.5017243857383726\n","\n","Epoch:   44/50    Loss: 3.4754963994026182\n","\n","Epoch:   45/50    Loss: 3.4738941175224136\n","\n","Epoch:   45/50    Loss: 3.4186794543266297\n","\n","Epoch:   45/50    Loss: 3.445701150417328\n","\n","Epoch:   45/50    Loss: 3.4290692830085754\n","\n","Epoch:   45/50    Loss: 3.4848779950141906\n","\n","Epoch:   45/50    Loss: 3.452557800769806\n","\n","Epoch:   45/50    Loss: 3.482955997467041\n","\n","Epoch:   45/50    Loss: 3.4827366971969607\n","\n","Epoch:   45/50    Loss: 3.501227361679077\n","\n","Epoch:   45/50    Loss: 3.497356852054596\n","\n","Epoch:   45/50    Loss: 3.470081109046936\n","\n","Epoch:   45/50    Loss: 3.49667564868927\n","\n","Epoch:   45/50    Loss: 3.487005455970764\n","\n","Epoch:   46/50    Loss: 3.465050211042259\n","\n","Epoch:   46/50    Loss: 3.428203323364258\n","\n","Epoch:   46/50    Loss: 3.4337354121208192\n","\n","Epoch:   46/50    Loss: 3.453909598350525\n","\n","Epoch:   46/50    Loss: 3.442171492099762\n","\n","Epoch:   46/50    Loss: 3.4833081283569336\n","\n","Epoch:   46/50    Loss: 3.4627824053764344\n","\n","Epoch:   46/50    Loss: 3.4575382018089296\n","\n","Epoch:   46/50    Loss: 3.4766896162033083\n","\n","Epoch:   46/50    Loss: 3.474856372356415\n","\n","Epoch:   46/50    Loss: 3.48008531332016\n","\n","Epoch:   46/50    Loss: 3.4779206595420837\n","\n","Epoch:   46/50    Loss: 3.5128373136520388\n","\n","Epoch:   47/50    Loss: 3.468923010592416\n","\n","Epoch:   47/50    Loss: 3.4382205877304077\n","\n","Epoch:   47/50    Loss: 3.442879528999329\n","\n","Epoch:   47/50    Loss: 3.4461698837280275\n","\n","Epoch:   47/50    Loss: 3.428446305274963\n","\n","Epoch:   47/50    Loss: 3.479646262168884\n","\n","Epoch:   47/50    Loss: 3.472190848350525\n","\n","Epoch:   47/50    Loss: 3.4509012360572813\n","\n","Epoch:   47/50    Loss: 3.4673501329421996\n","\n","Epoch:   47/50    Loss: 3.477234199523926\n","\n","Epoch:   47/50    Loss: 3.463487955570221\n","\n","Epoch:   47/50    Loss: 3.4852542662620545\n","\n","Epoch:   47/50    Loss: 3.462620978832245\n","\n","Epoch:   48/50    Loss: 3.4561571413583128\n","\n","Epoch:   48/50    Loss: 3.423913420200348\n","\n","Epoch:   48/50    Loss: 3.428621786117554\n","\n","Epoch:   48/50    Loss: 3.440072358608246\n","\n","Epoch:   48/50    Loss: 3.4489770455360413\n","\n","Epoch:   48/50    Loss: 3.4551590843200684\n","\n","Epoch:   48/50    Loss: 3.4677127656936646\n","\n","Epoch:   48/50    Loss: 3.459679711818695\n","\n","Epoch:   48/50    Loss: 3.4430208959579467\n","\n","Epoch:   48/50    Loss: 3.474889340877533\n","\n","Epoch:   48/50    Loss: 3.4565932383537294\n","\n","Epoch:   48/50    Loss: 3.495193106174469\n","\n","Epoch:   48/50    Loss: 3.4790544872283937\n","\n","Epoch:   49/50    Loss: 3.4385468917544366\n","\n","Epoch:   49/50    Loss: 3.4157895994186402\n","\n","Epoch:   49/50    Loss: 3.426647437572479\n","\n","Epoch:   49/50    Loss: 3.443575423717499\n","\n","Epoch:   49/50    Loss: 3.435864869117737\n","\n","Epoch:   49/50    Loss: 3.4348133091926574\n","\n","Epoch:   49/50    Loss: 3.452870725154877\n","\n","Epoch:   49/50    Loss: 3.4525351696014406\n","\n","Epoch:   49/50    Loss: 3.4912445101737974\n","\n","Epoch:   49/50    Loss: 3.4689285745620726\n","\n","Epoch:   49/50    Loss: 3.4614799385070802\n","\n","Epoch:   49/50    Loss: 3.4662434878349306\n","\n","Epoch:   49/50    Loss: 3.4796810183525086\n","\n","Epoch:   50/50    Loss: 3.4409434559198564\n","\n","Epoch:   50/50    Loss: 3.402374309062958\n","\n","Epoch:   50/50    Loss: 3.3975505261421204\n","\n","Epoch:   50/50    Loss: 3.4367927870750425\n","\n","Epoch:   50/50    Loss: 3.4446991982460022\n","\n","Epoch:   50/50    Loss: 3.4551877913475035\n","\n","Epoch:   50/50    Loss: 3.439871271133423\n","\n","Epoch:   50/50    Loss: 3.461858449459076\n","\n","Epoch:   50/50    Loss: 3.46321199464798\n","\n","Epoch:   50/50    Loss: 3.4642287893295287\n","\n","Epoch:   50/50    Loss: 3.4665957670211793\n","\n","Epoch:   50/50    Loss: 3.4644410810470583\n","\n","Epoch:   50/50    Loss: 3.46381662607193\n","\n","Model Trained and Saved\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n","  \"type \" + obj.__name__ + \". It won't be checked \"\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"BEzp_gvsgO7c","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}